{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the SCALE Mediciad Lab","text":"<p>This website provides onboarding documentation for new team members as well as documentation related to typical data sets we work with, best practices for research, and any additional data science related information we find useful. </p>"},{"location":"research_areas/","title":"Research Areas","text":"<p>Our group focuses on a few core research areas within the Medicaid program. This page provides a brief overview of those topics and provides links to our publications in each area.</p>"},{"location":"research_areas/#medicaid-managed-care","title":"Medicaid Managed Care","text":"<p>The overwhelming majority of the Medicaid program is administered through managed care arrangements in which the government makes standardized payments to private insurers (often for-profit) based on the number of beneficiaries those insurers cover. These insurers take on financial risk and stand to lose or profit if they administer health care services for more or less than the payments received from the government. </p> <p>Because the Medicaid benefit is fairly standardized, these insurers only have a limited set of tools they can use to find efficiencies. For example, federal and state regulations mandate services and drugs that must be covered within Medicaid, which is not the case for the commercial health insurance market. Furthermore, consumer cost sharing (copays, deductibles, co-insurance) is often $0 for Medicaid beneficiaries, or capped at a small amount that cannot be exceeded by any Medicaid plan within the state. </p> <p>Given this limited tool set, previous research in this area has focused on:</p> <ul> <li>Understanding whether managed care has actually reduced spending within Medicaid</li> <li>Understanding what effects, if any, plans have in this highly constrained market</li> <li>Understanding the mechanisms through which plans achieve savings or impact health</li> </ul> 2022 Managed Care Enrollment Rates by State <p></p>"},{"location":"research_areas/#related-publications","title":"Related Publications","text":"<ul> <li>The Private Provision of Public Services: Evidence from Random Assignment in Medicaid</li> <li>What Difference Does a Health Plan Make? Evidence from Random Plan Assignment in Medicaid</li> <li>The Impact of Medicaid Managed Care Plan Type on Continuous Medicaid Enrollment: A Natural Experiment</li> </ul>"},{"location":"research_areas/#medicaid-providers","title":"Medicaid Providers","text":""},{"location":"research_areas/#building-a-smarter-safety-net","title":"Building A Smarter Safety Net","text":""},{"location":"data_science/coding_conventions/","title":"Coding Conventions","text":"<p>These are some general guidelines that help maintain consistency across projects.</p>"},{"location":"data_science/coding_conventions/#naming-conventions","title":"Naming Conventions","text":"<ul> <li>Do not include the following characters in filenames, function names, or variable names: spaces, periods, most other special characters (underscores and dashes are okay)</li> <li>Keep names short but descriptive</li> <li>Keep naming conventions consistent within a project<ul> <li>Lowercase, separate words by an underscore<ul> <li><code>enrollment_table.csv</code></li> <li><code>claims_2012.parquet</code></li> </ul> </li> <li>CamelCase, separate words denoted by capilization<ul> <li><code>EnrollmentTable.csv</code></li> <li><code>Claims2012.parquet</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"data_science/coding_conventions/#python-specific-conventions","title":"Python-Specific Conventions","text":"<p>Python often has its own general style guidlines which are similar to the above, but vary depending upon what is being named. The general naming convention rules still apply with the following modifications.</p> <ul> <li>Functions: lowercase with words separated by an underscore<ul> <li><code>def grab_raw_claims()</code></li> </ul> </li> <li>Variables: lowercase with words separated by an underscore<ul> <li><code>num_days = 31</code></li> <li>Constants can be all caps with words separated by underscores: <code>DAYS_IN_WEEK = 7</code></li> </ul> </li> <li>Classes: CamelCase<ul> <li><code>pd.DataFrame</code></li> <li><code>class MedicaidEnrollee()</code></li> <li>Instances of a class: lowercase, same as variables<ul> <li><code>df = pd.DataFrame()</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"data_science/coding_conventions/#r-specific-conventions","title":"R-Specific Conventions","text":"<p>Many of the Python-specific conventions apply. A few additional ones:</p> <ul> <li>Avoid using restricted names for your objects (e.g., <code>data</code>)</li> <li>Track which libraries you actually need to load, and ensure you load them in a consistent order to avoid conflicts. Consider using the double-colon operator (e.g., <code>dplyr::filter()</code>)</li> <li>Your R scripts should not include calls to system-altering functions such as <code>install.packages()</code>. Consider using the <code>renv</code> package to manage dependencies</li> </ul> <p>We highly recommend referring to the Tidyverse Style Guide.</p>"},{"location":"data_science/open_data_resources/","title":"Open Data Resources","text":"<p>The references below offer classifications and registries to augment analyses.</p> Open Data Resource Purpose Avoidable Emergency Department (ED) Algorithm developed by NYU to classify ED visits into Non-emergent; Emergent/Primary Care Treatable; Emergent (ED Care Needed), Preventable/Avoidable; Emergent (ED Care Needed), Not Preventable/Avoidable. Chronic Condition Warehouse (CCW) The CCW groups diagnosis codes into well-known categories, such as Diabetes or Hypertension. There are 30 conditions in total. Clinical Classifications Software (CCS) ICD-10-CM procedure grouper with 224 classifications. Diagnosis-Related Groups (DRG) Diagnosis groupers, which are used to decide case mix for Medicare reimbursements. Healthcare Effectiveness Data &amp; Information Set (HEDIS) Performance quality with &gt;90 measures ranging from Effectiveness of Care, Access/Availability of Care, Experience of Care, Utilization &amp; Risk Adjusted Utilization, Health Plan Descriptive Information, and Measures Reported Using Electronic Clinical Data Systems. National Drug Code (NDC) Registry for drugs approved by the FDA. National Plan &amp; Provider Enumeration System (NPPES) Provider (and provider organization) registry developed by CMS. Prevention Quality Indicators (PQI) Inpatient quality measures that \"use data from hospital discharges to identify admissions that might have been avoided through access to high-quality outpatient care.\""},{"location":"data_science/parallelization/","title":"Parallelization using dSQ","text":""},{"location":"data_science/parallelization/#overview","title":"Overview","text":"<p>Parallelization is helpful when you need to run a time/memory intensive script many times or over many states.  Parallelization helps run multiple states at the same time to make code run quicker and more efficiently.  In this document, I will show how to convert a typical python script that loops over states into something that can be run in parallel.</p> <p>Note: Parallelization is, of course, helpful for more things than just running a bunch of states at once.  See this resource to learn more about dSQ and its many capabilities.</p>"},{"location":"data_science/parallelization/#basic-example","title":"Basic Example","text":"<p>When you write a python script use for parallelization, you structure your code slightly differently.  For example, a typical python script that loops over states might look something like this.</p> <pre><code>def my_function(state):\n    # Your processing code here\n    print(f\"Processing state: {state}\")\n\nstates = [\"AL\", \"AK\", \"AZ\", \"AR\", \"...\"] # etc.\n\nfor state in states:\n    my_function(state)\n</code></pre> <p>The same script written in a parallel-ready format would look like this: <pre><code>def my_function(state):\n    # Your processing code here\n    print(f\"Processing state: {state}\")\n\nif __name__ == \"__main__\":\n    # The state is now passed as a command-line argument\n    state_to_process = sys.argv[1]\n    my_function(state_to_process)\n</code></pre></p> <p>This needs to be used in conjunction with a <code>joblist.txt</code> file that looks like this (assuming your parallel-ready .py file is called my_script.py)</p> <pre><code>python my_script.py AL\npython my_script.py AK\npython my_script.py AZ\npython my_script.py AR\n# ... and so on for all your states\n</code></pre> <p>In our context, you'd want to load miniconda and your environment too, so it might look something like this</p> <pre><code>module load miniconda; conda activate my_env; python my_script.py AL\nmodule load miniconda; conda activate my_env; python my_script.py AK\n# ... etc.\n</code></pre> <p>Next, you want to go to your command line connected to the YCRC clusters and load dSQ onto your path with <code>module load dSQ</code>.  Then we want to tell dSQ to make our bash script.  For example, we might run <code>dsq --job-file joblist.txt --mem-per-cpu 4g -t 20:00 --mail-type ALL</code> to request 4 gigs and 20 minutes per job.  See more on the settings you can specify here.  This will create a script called <code>dsq_joblist-yyyy-mm-dd.sh</code> where yyyy-mm-dd is today's date.  The script will look something like this</p> <p><pre><code>#!/bin/bash\n#SBATCH --array 0-999\n#SBATCH --output dsq-joblist-%A_%3a-%N.out\n#SBATCH --job-name dsq-joblist\n#SBATCH --mem-per-cpu 4g -t 20:00 --mail-type ALL\n\n# DO NOT EDIT LINE BELOW\n/path/to/dSQBatch.py --job-file /path/to/joblist.txt --status-dir /path/to/here\n</code></pre> You can edit this file as desired.  If you get rid of the <code>-%A_%3a-%N</code> in the <code>.out</code> file, it will put all the output in one file instead of creating a separate file for each job.</p> <p>Finally, make sure all of these files (<code>my_script.py</code>, <code>joblist.txt</code>, and <code>dsq_joblist-yyyy-mm-dd.sh</code>) are in your desired folder and submit your batch script using <code>sbatch dsq-joblist-yyyy-mm-dd.sh</code>.</p> <p>If you need more customization or run into errors, YCRC has some great suggestions of how to start and diagnose.</p> <p>Note: Unfortunately, there is not a way to customize how much memory each job gets, meaning if you run all states in one script you'd have to assign CA and HI the same amount of memory.  One potential workaround is to create multiple scripts \u2014 a \"big state\" and \"small state\" script, for example.</p>"},{"location":"data_science/parallelization/#another-example","title":"Another Example","text":"<p>Below is the group of files I made to run a very simple function that imports the demographic file and reports the number of unique <code>BENE_ID</code>s.  This implementation is for five states, but obviously could be run for more. <code>parallel_test.py</code> <pre><code>import pyarrow.parquet as pq\nimport pyarrow.compute as pc\nimport pyarrow as pa\nimport sys\n\np = '/home/as4765/medicaid_lab/data/cms/ingested/TMSIS_taf/taf_demog_elig_base/'\nyears = range(2016,2022)\n\ndef function(state):\n\u00a0 \u00a0 print(f'Processing state: {state}')\n\u00a0 \u00a0 for year in years:\n\u00a0 \u00a0 \u00a0 \u00a0 schema = pq.read_schema(f'{p}year={year}/state={state}/data.parquet')\n\u00a0 \u00a0 \u00a0 \u00a0 new_schema = schema.set(schema.get_field_index('year'), pa.field('year', pa.int64()))\n\n\u00a0 \u00a0 \u00a0 \u00a0 table = pq.read_table(f'{p}/year={year}/state={state}/data.parquet',\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0   columns = ['BENE_ID'],\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 schema = new_schema)\n\u00a0 \u00a0 \u00a0 \u00a0 unique_ids = pc.unique(table['BENE_ID'])\n\u00a0 \u00a0 \u00a0 \u00a0 print(f'Year: {year}, State: {state}, Rows: {len(unique_ids)}')\n\nif __name__ == \"__main__\":\n\u00a0 \u00a0 state_to_process = sys.argv[1]\n\u00a0 \u00a0 function(state_to_process)\n</code></pre></p> <p><code>joblist_parallel_test.txt</code> <pre><code>module load miniconda; conda activate test_environ; python3 ./parallel_test.py AZ\nmodule load miniconda; conda activate test_environ; python3 ./parallel_test.py HI\nmodule load miniconda; conda activate test_environ; python3 ./parallel_test.py MO\nmodule load miniconda; conda activate test_environ; python3 ./parallel_test.py NH\nmodule load miniconda; conda activate test_environ; python3 ./parallel_test.py WA\n</code></pre> The following batch script is made using the following prompt: <code>dsq --job-file joblist_parallel_test.txt --mem-per-cpu 20g -t 20:00 --mail-type ALL</code></p> <p><code>dsq-joblist_parallel_test-2025-06-26.sh</code> <pre><code>#!/bin/bash\n#SBATCH --output dsq-joblist_parallel_test.out\n#SBATCH --array 0-4\n#SBATCH --job-name dsq-joblist_parallel_test\n#SBATCH --mem-per-cpu 20g -t 20:00 --mail-type ALL\n\n# DO NOT EDIT LINE BELOW\n/gpfs/milgram/apps/hpc.rhel7/software/dSQ/1.05/dSQBatch.py --job-file /gpfs/milgram/project/ndumele/as4765/misc/joblist_parallel_test.txt --status-dir /gpfs/milgram/project/ndumele/as4765/misc\n</code></pre></p>"},{"location":"data_science/parallelization/#considerations-in-r","title":"Considerations in R","text":"<p>The theory behind parallel processing is the same in R, but the tools are different. The developers of <code>purrr</code> are working on a function <code>purrr::in_parallel()</code> to facilitate parallelization of loops. In the meantime, you can use the <code>furrr</code> package. If you're building a <code>targets</code> pipeline, follow their parallelization guide.</p>"},{"location":"data_science/project_organization/","title":"Project Organization","text":"<p>Research projects are typically lengthy, require several revisions, and often outlast the tenure of the initial primary researcher. By maintaining consistency across projects, other researchers can readily contribute to revisions or new analyses for a project they did not initially work on. The following should act as general guidelines which have proven useful in the past and can be adapted.</p>"},{"location":"data_science/project_organization/#user-directories","title":"User Directories","text":"<p>Within the group's directory, each researcher should create their own folder to have a dedicated place to work.</p> <ul> <li>Yale Server: <code>D:/Groups/YSPH-HPM-Ndumele/Networks/</code></li> <li>Milgram: <code>/gpfs/milgram/project/ndumele/</code> </li> </ul> <pre><code>Networks/\n\u251c\u2500\u2500 Anthony/\n\u251c\u2500\u2500 Beniamino/\n\u251c\u2500\u2500 Hannah/\n\u251c\u2500\u2500 Dohyun/\n...\n\u251c\u2500\u2500 Jacob/\n</code></pre> <p>Within your own folder, you can create a separate folder for each distinct project. Choose a short, descriptive name so another researcher would easily know what research project the folder relates to.</p> <pre><code>Networks/\n\u251c\u2500\u2500 Anthony/\n    \u251c\u2500\u2500 Network_Overlap/\n    \u251c\u2500\u2500 LA_Plan_Effects/\n    \u251c\u2500\u2500 TN_Plan_Effects/\n    \u251c\u2500\u2500 MI_enrollment/\n</code></pre> <p>Within each project it helps to organize the code in one spot and the analytic products in another. The code is the most important aspect, as you want another researcher to readily know which code is responsible for all of the analyses. For instance, within the <code>MI_enrollment</code> folder the overall organization would look something like:</p> <pre><code>MI_enrollment/\n\u251c\u2500\u2500 README.txt\n\u251c\u2500\u2500 venv/\n\u251c\u2500\u2500 code/\n    \u251c\u2500\u2500 main.py\n    \u251c\u2500\u2500 analysis.py\n    \u251c\u2500\u2500 enrollment_counts.py\n    \u251c\u2500\u2500 sensitivity_analysis.py\n    \u251c\u2500\u2500 project_paths.py\n\u251c\u2500\u2500 trunk/\n    \u251c\u2500\u2500 raw/\n        \u251c\u2500\u2500 grab_MI_claims/\n            \u251c\u2500\u2500 claims_2010.pkl\n            \u251c\u2500\u2500 claims_2011.pkl\n            ...\n            \u251c\u2500\u2500 diagnosis_2016.pkl\n    \u251c\u2500\u2500 derived/\n        \u251c\u2500\u2500 create_enrollment_table/\n            \u251c\u2500\u2500 output/\n                \u251c\u2500\u2500 enrollment_table.pkl\n        \u251c\u2500\u2500 create_pregnancy_list/\n            \u251c\u2500\u2500 output/\n                \u251c\u2500\u2500 pregnancy.pkl\n    \u251c\u2500\u2500 analysis/\n        \u251c\u2500\u2500 hazard_plot/\n            \u251c\u2500\u2500 output/\n                \u251c\u2500\u2500 enrollment_distribution.png\n                \u251c\u2500\u2500 hazard_by_age_monthly.png\n        \u251c\u2500\u2500 stability_DiD_estimates/\n            \u251c\u2500\u2500 temp/\n            \u251c\u2500\u2500 output/\n                \u251c\u2500\u2500 coeffs.csv\n\u251c\u2500\u2500 jupyter/ (python-specific)\n    \u251c\u2500\u2500 sandbox.ipynb\n    \u251c\u2500\u2500 enrollment_check.ipynb\n\u251c\u2500\u2500 .gitattributes\n\u251c\u2500\u2500 .gitignore\n</code></pre> <ul> <li><code>README.txt</code>: The first file someone should read, it contains any particular project-specific information or organization that is important to know and documents how to reproduce the analyses fully from start to finish.</li> <li><code>venv/</code>: The virtual environment for the project to ensure that we don't run into library versioning issues across projects.</li> <li><code>code/</code>: Contains all code related to the project.<ul> <li><code>main.py</code>: A single script which can re-run all of the analyses from start to finish. For instance, in python this script might import other .py files and would have a single function that, if run, would reproduce all analyses from start to finish.</li> <li><code>other_files.py</code>: Individual code files which are responsible for specific analyses. For instance, it may make sense to have a different file for pulling copies of the raw data, one for the creation of the analytic table from the raw data, and another for sensitivy analyses.</li> </ul> </li> <li><code>trunk/</code>: This folder contains all data, derived tables, and analytic extracts for this project.<ul> <li><code>raw/</code>: An optional folder. Would contain copies of Medicaid data which are pulled into this specific project. The raw data can be accessed directly from the Storage at Yale drive (YSPH_HPM-CC0940-MEDSPH) or on Milgram (<code>/gpfs/milgram/pi/medicaid_lab/data/</code>). This is useful if you need a targeted or smaller subset of data to work from.</li> <li><code>derived/</code>: This folder contains any datasets which are created specifically for this project. Within <code>derived/</code>, each folder represents some atomic manipuation or derivation, which may be used in other processes or to create analytic products. The outputs stored in these directories are almost always data files (.pkl, .parquet).<ul> <li>In python, the method responsible for running the analysis should share the same name as the folder. For instance, the method <code>create_enrollment_table()</code> would be responsible for generating <code>create_enrollment_table/output/enrollment_table.pkl</code>, making it easy for anyone to identify the code responsble.</li> </ul> </li> <li><code>analysis/</code>: This folder contains the analytic endpoints, generally from analyses of some data produced in <code>derived/</code>. The outputs stored in these directories are typically figures, tables, or smaller data files containing coefficients or data that would create a manuscript table.</li> </ul> </li> <li><code>jupyter/</code>: A potential sandbox to test and develop code. (In an R project, this could be called <code>qmd</code>, <code>rmd</code>, <code>notebooks</code>, or something similar.) Should be treated as temporary.</li> <li><code>.gitfiles</code> and <code>.gitignore</code>: If the folder is version controlled with git. DATA SHOULD NEVER BE PUSHED. (Also avoid pushing any information that could recreate raw data.) Because the code exists in one directory, you can ignore <code>trunk/</code> and only maintain <code>code/</code>.</li> </ul>"},{"location":"data_science/project_organization/#project-templates","title":"Project Templates","text":"<p>We recommend starting your R projects from the template repository on the Yale Medicaid Lab GitHub page. Follow the instructions in the README file in that repository.</p>"},{"location":"data_science/project_organization/#project-tracking","title":"Project Tracking","text":"<p>As our team grows, project management has become more and more important. Chima, Jacob, and Anthony are involved in many workstreams, so creating a centralized tracker for your project will help everyone stay on the same page. If you have a system that works for you, great! Otherwise, see below for our recommendation.</p> <p>Many members of the lab use GitHub Projects as a project management tool. GitHub Projects is a Kanban board that integrates with your repository. Each \"ticket\" you create on Projects syncs with a corresponding \"issue\" on the GitHub repository. Branches also align with your ticket, so when you merge a pull request the ticket moves to the \"done\" column.</p>"},{"location":"data_science/venv/py_venv/","title":"Python Virtual Environments","text":"<p>The Python Packaging User Guide contains step-by-step instructions for creating and using a virtual environment. The steps below are explicit to one way to do so on our server.</p>"},{"location":"data_science/venv/py_venv/#creating-a-new-virtual-environment","title":"Creating a New Virtual Environment","text":"<p>To set up a virtual environment for a project you can navigate to the project folder (for instance <code>Anthony/NewProject/</code>) and activate GitBash. (If using the file explorer, right click and select <code>GitBash Here</code>.) This will open GitBash and you should see the following:</p> <pre><code>your_netID@ysph-hpm MINGW64 /d/Groups/YSPH-HPM-Ndumele/Networks/Anthony/NewProject\n$\n</code></pre> <p>Create a new virtual environment by typing:</p> <pre><code>python -m venv my_venv\n</code></pre> <p>This will create a virtual environment in the folder named <code>my_venv</code>; if you would like it named something different, you can change the name of the last argument.</p>"},{"location":"data_science/venv/py_venv/#activating-and-adding-packages-to-the-virtual-environment","title":"Activating and Adding Packages to the Virtual Environment","text":"<p>Now that a virtual environment has been created, we can activate the virtual environment (named <code>my_venv</code>) with GitBash by typing:</p> <pre><code>source my_venv/Scripts/activate\n</code></pre> <p>After this is done you will know it's successful because GitBash will indicate the environment being used within <code>()</code>:</p> <pre><code>(my_venv)\nal686@ysph-hpm MINGW64 /d/Groups/YSPH-HPM-Ndumele/Networks/Anthony/NewProject\n$\n</code></pre> <p>Packages can then be installed into this virtual environment with pip, so to install the latest version of numpy we would just type <code>pip3 install numpy</code>. The packages will then be saved to <code>.../my_venv/Lib/site_packages/</code>.</p>"},{"location":"data_science/venv/py_venv/#setting-up-jupyter-notebooks","title":"Setting up Jupyter Notebooks","text":"<p>Jupyter notebooks are great for exploratory data analysis or code development. In order to run jupyter notebooks we will need to install <code>ipykernel</code> and pass the virtual environment to the name. This is accomplished with the following two commands:</p> <pre><code>pip3 install ipykernel\n\npython -m ipykernel install --user --name=my_venv\n</code></pre> <p>Then, open a juptyer notebook within GitBash with <code>jupyter notebook</code>. Now when you create a new notebook you will be able to select this virtual environment when starting a new notebook:</p> <p></p>"},{"location":"data_science/venv/py_venv/#deactivating-the-virtual-environment","title":"Deactivating the virtual environment","text":"<p>Typing <code>deactivate</code> will deactivate the currently activated virtual environment.</p>"},{"location":"data_science/venv/r_venv/","title":"Virtual Environments in R","text":"<p>The most common way to use a virtual environment in R is through the <code>renv</code> package, developed by Posit. The best place to learn about using <code>renv</code> is the package's introductory vignette here, which provides a brief overview of the package and everything you need to get started.</p> <p>You can also see examples of <code>renv</code> being used in practice in many of our R codebases on the lab GitHub, including:</p> <ol> <li>The physician to voter codebase, which uses <code>renv</code> to ensure reporoducibility and make sure that it will still run even if a server user updates their package libraries.</li> <li>The TMSIS ingestion pipeline, which uses <code>renv</code> to help ensure that all maintainers are using the same version of the dependency packages.</li> </ol>"},{"location":"data_science/venv/venv_overview/","title":"Overview","text":"<p>Virtual environments are useful tools that allow you to maintain a seperate set of packages for each project you are working on. This approach has several advantages over maintaining a central package installation common to all projects:</p> <ol> <li>Virtual environments allow you to clearly communicate your code's required dependencies to your collaborators, reducing the time needed to onboard new collaborators.</li> <li>Virtual environments allow you to keep seperate versions of packages for each project, so you can use software only compatible with a specific version of a package.</li> <li>Updating dependencies for one project will never break dependencies for another project.</li> </ol>"},{"location":"embedding/coding_best_practices/","title":"Coding Best Practices","text":""},{"location":"embedding/coding_best_practices/#respectful-engineering-practices","title":"Respectful Engineering Practices","text":"<p>Warning</p> <p>Engineering in embedded environments should prioritize respect toward the external systems and our partners that manage them. Avoid intensive computing, clever work-arounds, and unfamiliar code as much as possible.</p> <p>Our team offers flexible workflows based on individual preferences (e.g., Python or R for analysis). However, in embedded systems, we have the privilege\u2014and responsibility\u2014to work respectfully in external environments. \"Respectful\" coding practices typically involve foregoing some flexibility to be good partners. This includes...</p> <ol> <li>Avoid resource-intensive computation<ul> <li>Avoid multi-hour queries without proper oversight and alignment.</li> <li>Avoid creating large datasets (&gt;500 MB) without proper oversight and alignment. An example for checking table size in Oracle is available below.</li> </ul> </li> <li>Avoid implementing clever work-arounds<ul> <li>Avoid overriding administrative or security constraints to run programs.</li> <li>Avoid installing new software as much as possible.</li> <li>Avoid command line processes needed to run a program.</li> </ul> </li> <li>Avoid using unfamiliar code<ul> <li>Avoid copy-pasting code from online (e.g., StackOverflow or ChatGPT).</li> <li>Avoid running code from colleagues without proper instruction.</li> </ul> </li> <li>Avoid cluttering shared resources<ul> <li>Remove temporary datasets or outdated database tables.</li> <li>Use the shared schema (e.g., <code>YALE</code>) rather than your user schema, which is harder to track and manage as a team.</li> </ul> </li> </ol> <p>While the list above may not be comprehensive, it should orient you to the mindset needed while programming within embedded systems. If\u2014after adopting this mindset\u2014any engineering activities give you pause, then do not hesitate to reach out to research supervisors with questions.</p>"},{"location":"embedding/coding_best_practices/#masking-small-cell-sizes","title":"Masking Small Cell Sizes","text":"<p>As discussed elsewhere, when working in embedded (administrative) systems, we must be cautious about sensitive datasets (PII/PHI). To protect the privacy and well-being of individuals in these systems, we need to avoid \"re-identification risks\" when reporting information, even once it's been aggregated. Specifically, we opt to mask data when reporting info for groups with fewer than 11 individuals. We present an example below:</p> Before maskingAfter masking <p>Consider a geographic analysis at the town-level. When we initially pull the data, we find that some of the reporting categories have fewer than 11 data points (Pomfret).</p> Town Members Total Paid Hartford 93 $87,913 New Haven 127 $132,333 Pomfret 9 $9,171 Windsor 21 $23,930 <p>To avoid any risks in re-identifying members in Pomfret, we opt to mask the details related to this subgroup. While we can still mention that the town was considered (i.e., included in the \"Town\" column), we remove the details for reporting purposes. This data may remain in subsequent aggregate analyses.</p> Town Members Total Paid Hartford 93 $87,913 New Haven 127 $132,333 Pomfret NA NA Windsor 21 $23,930 <p>We provide some sample code to perform this masking process, either in R or Python. These functions can be repurposed, edited, and updated as seen fit to accommodate your project needs.</p> RPython <pre><code># When reporting aggregates, mask all numeric columns in a dataframe (df) based on a\n# count column (mask_based_on) that falls below a certain threshold (mask_threshold)\n# &gt;&gt;&gt; Example usage: data |&gt; mask_small_cells(mask_based_on = member_count_col) |&gt; ...\nmask_small_cells &lt;- function(df, mask_based_on, mask_threshold = 10) {\n    dplyr::mutate(\n        df,\n        dplyr::across(dplyr::where(is.numeric),\n        \\(x) ifelse({{mask_based_on}} &lt;= mask_threshold, NA, x))\n    )\n}\n</code></pre> <pre><code># [Requires pandas and numpy (np)]\n# When reporting aggregates, mask all numeric columns in a pandas dataframe (df) based on\n# a count column (mask_based_on) that falls below a certain threshold (mask_threshold)\n# &gt;&gt;&gt; Example usage: masked_df = mask_small_cells(df, mask_based_on = member_count_col)\ndef mask_small_cells(df, mask_based_on, mask_threshold = 10):\n    data = df.copy()\n    data.loc[\n            data[mask_based_on] &lt;= mask_threshold,\n            data.select_dtypes(include=np.number).columns.tolist()\n    ] = np.nan\n    return(data)\n</code></pre>"},{"location":"embedding/coding_best_practices/#oracle-table-size","title":"Oracle Table Size","text":"<p>The script below reports the size of every table for a schema (e.g., <code>YALE</code> below). Among other limitations, this script does not report information for temporary tables, which should rarely be used.</p> <pre><code>SELECT\n  ALL_TABLES.TABLE_NAME,\n  SUM(USER_SEGMENTS.BYTES) / 1024 / 1024 AS MB\nFROM\n  ALL_TABLES\n  INNER JOIN USER_SEGMENTS\n    ON ALL_TABLES.TABLE_NAME = USER_SEGMENTS.SEGMENT_NAME\nWHERE\n  ALL_TABLES.OWNER = 'YALE'\nGROUP BY\n  ALL_TABLES.TABLE_NAME\nORDER BY\n  ALL_TABLES.TABLE_NAME\n;\n</code></pre>"},{"location":"embedding/guidebook/","title":"Embedding Guidebook","text":"<p>The Embedding Guidebook is a resource for staff members that will have access to work within external data systems, such as direct access to a state agency database. This documentation does not include specific onboarding tasks for embedding. Instead, the guidebook will help you navigate working across firewalled systems.</p> <p>Please read closely and familiarize yourself with the unique constraints for embedded work. We include specific guardrails and general guidelines for reference. Nevertheless, the number one guideline for embedded work is ask whenever you are uncertain. In the meantime, there are a few main themes to keep in mind that appear throughout the guidebook:</p> <ul> <li>Compliance and regulations: The partnering organization will have their own set of rules (e.g., related to data access and privacy). Receiving credentials to access external systems may involve completing forms or trainings for these subjects. In addition to these organizational trainings, we provide more role-specific rules here.</li> <li>Pre-approved scope of work: Accessing data (both at Yale and externally) has been pre-approved for specific research activities. While this guidebook outlines how we operate across systems at a high-level, you may benefit from reviewing the scope of work documents related to your projects. The scope may be documented in a Research Service Agreements or IRB approval.</li> <li>Knowledge sharing: In addition to data access, you will likely have transparency into the partnering organization\u2019s internal operations. Accessing sensitive information should be approached with just as much prudence as accessing sensitive data.</li> </ul>"},{"location":"embedding/guidebook/#navigating-across-firewalls","title":"Navigating Across Firewalls","text":"<p>Embedded analysts get to wear two hats as both a researcher at Yale and an applied analyst in their embedded role. Maybe contrary to expectations, the purpose of embedding is not to perform research. Instead, it augments the research we pursue at Yale. By supporting government partners in improving programs and policies, we can spot opportunities for new research or shape our current research with new perspectives and knowledge.</p> <p>In the diagram below, we visualize the importance of treating the systems as isolated environments with distinct activities performed in each.</p> <p></p> <p>Taking an example from the diagram above. In the partnering organization's system, we may support efforts to link important data across programs or departments. The linkage may inform administrative decisions and policies. By helping make this tool a reality, it also creates an opportunity to request newly available data linkages for continued research at Yale.</p>"},{"location":"embedding/guidebook/#which-projects-belong-in-which-systems","title":"Which projects belong in which systems?","text":"<p>All research intended for publication or grant requirements should be performed at Yale using deidentified data extracts. In comparison, applied support for policies, programs, and technical tooling is performed in the external system. The applied support may involve generating evidence to support decision-making, which can feel similar to pure research. We view this work to diverge toward quality improvement activities, which Lynn et al. (2007) define (in the health care setting) as, \"systematic, data-guided activities designed to bring about immediate improvement in health care delivery in particular settings.\"</p> <p>Any single project you support should not be divided across systems. As a result, we need to be intentional about where we initially pursue projects to avoid future duplication. Nevertheless, we expect some information will need to be duplicated when it\u2019s (1) appropriate to do so, and (2) relevant for both Yale and the embedded organization (e.g., high-level strategic documentation used for decision-making).</p>"},{"location":"embedding/guidebook/#which-info-can-be-shared-across-systems","title":"Which info can be shared across systems?","text":"<p>As we know, transferring data across systems is not allowed. But what about transferring information? (When discussing \"information\" below, we assume it is not disaggregated data.)</p> <p>For information flowing from Yale, it's commonly the case that relevant information will be shareable to empower government powers. However, some situations may still require additional thought, such as sharing preliminary results from separate grant projects.</p> <p>For information gathered through embedded projects, we err on the side of caution by sharing only what's needed. This is to protect both ourselves and our partners from unintended disclosures, privacy violations, or compliance issues. \"Only what's needed\" extends the \"minimum data required\" mindset that's also expected in our role. When running analysis, you know which data's required for you personally to do your job. For information sharing, we need to consider which info our non-embedded collaborators need (e.g., Yale Principal Investigators) to support appropriately.</p> <p>Common items that are okay to share include information related to your project management, high-level constraints, high-level blockers, and deliverable drafts (by which time all data's aggregated). Sharing information like this typically happens over meetings or emails. It does not involve signing into Yale-credentialed applications while embedded to centralize project management or to share files across the firewall.</p> <p>Items that you may encounter that are typically not shared from external systems include contractual language, internal memos, or code. If there are instances you think it's appropriate to do so, you should receive approval first. And, per the number one guideline, ask whenever you are uncertain.</p>"},{"location":"embedding/guidebook/#guardrails-dos-donts","title":"Guardrails: Do\u2019s &amp; Don\u2019t\u2019s","text":"<p>We expect everyone to conduct themselves professionally within external systems, similar to Yale systems. However, some daily tasks that typically seem harmless should be approached with more caution in external systems, especially when it\u2019s a government system. We highlight some of these items below.</p> Do\u2019s Don\u2019t\u2019s Compliance Trainings Do complete the external organization\u2019s HR checklist required for embedded personnel Do not assume the trainings are exactly the same as Yale Applications Do become familiar with the tools and applications made available with embedded credentials Do not log into online applications using your Yale or personal credentials (e.g., Google, Outlook, Asana, etc.) Installations Do know how to request new software through the partnering organization\u2019s support teams Do not install new software without prior approval Email &amp; Messaging Do know that any messages sent on government systems can be viewed by the public through Freedom of Information Act (FOIA) requests Do not send code, data, internal findings, or other sensitive information (e.g., contractual info) to your personal or Yale accounts Document Access Do use organizational trainings, documentation, and reports to perform your role as expected Do not access documents that are not relevant to your work Data Requests Do understand that you have skills and data access that are useful to many groups, both internal and external to the partnering organization Do not provide data access or data extracts to anyone outside our team without express written approval from a supervisor at the partnering organization Contractual Information Do know that the partnering organization may work with multiple contractors relevant to your work Do not share information about contractual agreements or contractual language outside the embedded organization without approval from a supervisor at the partnering organization"},{"location":"embedding/onboarding/","title":"Embedding Onboarding","text":"<p>Most information related to embedded systems will be tracked within the embedded portal (virtual machine). Below, we track how to establish a VM connection and debug any administrative issues.</p> Support Team Contact Info When to use DSS IT Support 860-424-4949; DSS-ITS-Support@ct.gov Locked out of portal, password issues, questions about online IT tickets, ... DSS Security While in-person, ask to call the 2nd floor security to fix badge access Issues accessing Hartford Central Office in-person"},{"location":"embedding/onboarding/#azure-virtual-desktop-avd","title":"Azure Virtual Desktop (AVD)","text":"<p>The Connecticut DSS embedded portal is based out of an AVD instance. You must go through the following process to obtain online access:</p> <ol> <li>Have a team manager submit a DSS request for credentials</li> <li>Install the \"Microsoft Authenticator\" application on your phone for 2-factor authentication</li> <li>Install \"Microsoft Remote Desktop\" from the App Store<ul> <li>Upon launch, click the \"Microsoft Remote Desktop &gt; Preferences...\" button on the upper right console and unclick \"Enable optimizations for Microsoft Teams\"</li> <li>Go to \"Connections &gt; Add Workspace\" and use the following URL: https://rdweb.wvd.microsoft.com/api/arm/feeddiscovery</li> <li>Use your state credentials (@ct.gov) to log in, including 2-factor authentication</li> </ul> </li> <li>Reset your password<ul> <li>Review password requirements in the section below</li> <li>Directly from the VM display (Ctrl+Alt+Delete, or Ctrl+Opt+Delete on Mac)</li> <li>Alternatively, try going to office.com in the browser, logging in with your state credentials, and selecting \"Change password\" from the settings icon</li> </ul> </li> <li>In the AVD File Explorer, go to the 'DSS-Yale-Share/' folder in the shared network drive: <code>\\\\stavddss001.privatelink.file.core.windows.net\\dss-yale-share-001</code><ul> <li>You can put the link directly in the File Explorer file path (top-middle section).</li> </ul> </li> </ol>"},{"location":"embedding/onboarding/#dss-password-requirements","title":"DSS Password Requirements","text":"<p>The following info is specific to your DSS log-in account. Other credentials (e.g., database passwords) are managed separately.</p> <p>Connecticut DSS security requires the following password requirements:</p> <ul> <li>Must be at least 9 characters long</li> <li>Must contain at least one character in 3 of the following groups of characters:<ul> <li>Uppercase letters (A-Z)</li> <li>Lowercase letters (a-z)</li> <li>Numbers (0-9)</li> <li>Symbols (!@#$%^&amp;*)</li> </ul> </li> <li>Cannot match any of your previous 24 passwords</li> <li>Avoid using single words that can be found in the dictionary (e.g., \"Wintertime1!\")</li> <li>You cannot change your password more often than once in a 7-day period unless overridden by the helpdesk</li> </ul>"},{"location":"embedding/onboarding/#oracle-password-change","title":"Oracle Password Change","text":"<p>If you are locked out, reach out to DSS IT and request Gainwell support to unlock your MDW credentials.</p> <p>Changing your Oracle database password for the Medicaid Data Warehouse (MDW) can be done within any SQL editor, such as DBeaver. However, this requires setting the connection using only your username and excluding the <code>[YALE]</code> suffix, which is used to connect to the shared schema.</p> <p>For example, the following workflow may be performed in DBeaver with an existing connection:</p> <ul> <li>Right-click on the Oracle connection</li> <li>Click \"edit connection,\" changing the username to exclude the <code>[YALE]</code> suffix</li> <li>Save the edited connection, then right-click on the connection again (don't click \"edit connection\" this time)</li> <li>Select \"Security\" &gt; \"Change user password.\" Click save</li> <li>Right-click on the connection, click \"edit connnection,\" update the password, and reset the username to include <code>[YALE]</code> again</li> <li>Test connection and save updated credentials</li> </ul> <p>Your updated password should persist all Oracle connections, regardless of other connections (e.g., RStudio or Python). Make sure you keep a secure record of your passwords for tracking.</p>"},{"location":"onboarding/data_and_systems/","title":"Data and Systems Overview","text":"<p>We primarily work with administrative Medicaid data across three separate systems, depending upon the dataset and research project. Navigate to the navbar section about each dataset to learn more.</p>"},{"location":"onboarding/data_and_systems/#de-identified-individual-level-state-medicaid-data","title":"De-identified Individual-level State Medicaid Data","text":"<p>We have historic extracts of administrative claims, eligibility, and enrollment data from several states. Most of this data is old, with the Connecticut data being the most recent and the only data set we are actively refreshing. These data are stored at Yale on our Windows-based server, which you will access via Microsoft Remote Desktop. Click here to navigate to the \"State Medicaid Data\" tab.</p>"},{"location":"onboarding/data_and_systems/#national-de-identified-individual-level-state-medicaid-data","title":"National De-identified Individual-level State Medicaid Data","text":"<p>We have recent extracts of national Medicaid data provided by the Centers for Medicare and Medicaid Services (T-MSIS TAF). Our extract currently runs from 2016-2021 since there is a 2-3 year lag before the files are released in their final version. Due to the size of the data, they are stored on Milgram, a High Performance Computing (HPC) system at Yale managed by the Yale Center for Research Computing (YCRC). Click here to navigate to the \"T-MSIS\" tab.</p>"},{"location":"onboarding/data_and_systems/#individual-level-connecticut-medicaid-data","title":"Individual-level Connecticut Medicaid Data","text":"<p>Through a contract with the state of Connecticut, members of our team can obtained embedded access to Department of Social Services (DSS) systems to complete mutually agreed upon projects. After obtaining credentials, these data are accessed through an Azure Virtual Desktop (AVD), with all analyses and data remaining entirely within DSS systems. Data include identifiable individual-level administrative Medicaid claims, enrollment, eligibility, and various other program-specific datasets (e.g., Long Term Services and Supports Assessment). Click here to navigate to the \"CT DSS Embedded Materials\" tab.</p>"},{"location":"onboarding/new_member_onboarding/","title":"Welcome","text":"<p>Welcome to the SCALE (State Collaborations to Advance LEarning) in Medicaid Lab! We're excited to have you on the team and look forward to working with you on the problems that matter most to the Medicaid program. This page provides all the resources you need to get up and running on our projects, understand our workflow, and learn about how we build and deploy software!</p>"},{"location":"onboarding/new_member_onboarding/#administrative-steps","title":"Administrative Steps","text":"<p>There are several administrative steps to getting started that you may complete before you start:</p> <ol> <li>Get your Yale netID and e-mail set up.<ul> <li>Send an e-mail to Anthony with your netID and Yale e-mail address. We will use this to give you access to data and invite you to all of the relevant workspaces.</li> <li>Check with Anthony to schedule an appointment with Dan Holland to set up your laptop and to get access to our server. You will need Chima to approve access to Dan Holland for the following groups: <ul> <li>Yale\\ysph-hpm-rdpusers (for RDP access)</li> <li>Yale\\ysph-hpm-ndumele (for access to Chima\u2019s share)</li> <li>storage@yale group: YSPH_HPM-CC0940-MEDSPH2 (where HIPAA Medicaid data is stored)</li> </ul> </li> <li>When setting up your managed computer, request that ITS install the following programs:<ul> <li>Cisco AnyConnect Secure Mobility Client</li> <li>Microsoft Remote Desktop Windows App</li> <li>Slack</li> <li>Dropbox</li> <li>Zoom</li> </ul> </li> <li>If these are not installed on your computer, or if there are other programs you need, e-mail Dan Holland and we can get ITS to install them on your computer. </li> </ul> </li> <li>Complete all certifications and/or trainings required by Yale. To determine what trainings you need to complete, navigate here.<ul> <li>You need to complete:<ul> <li>Human Subject Protection Training </li> <li>HIPAA Privacy and Security for Researchers </li> <li>Yale University HIPAA Security Attestation</li> </ul> </li> </ul> </li> <li>Complete I9 verifications and related Workday HR administrative tasks that are required within your first week. </li> <li>Create various accounts that allow us to work collaboratively and remotely. For certain things, if you already have an account linked to your personal e-mail feel free to use that, or create another one using your Yale e-mail address.<ul> <li>GitHub: code version control and data science collaboration</li> <li>Dropbox: group collaboration </li> <li>Overleaf manuscript collaboration</li> </ul> </li> <li>Reach out to Anthony to make sure you are part of the YML Google Drive folder and relevant Dropbox folders. </li> <li>Set up a weekly 1:1 with Anthony. </li> </ol>"},{"location":"onboarding/new_member_onboarding/#getting-to-know-the-team-and-projects","title":"Getting to Know the Team and Projects","text":"<ol> <li>If you aren't familiar with the basics of the Medicaid program review this. Ask Anthony for additional resources if you would like more to read.</li> <li>Meet with Jacob and Chima to discuss the projects you'll be working on, why they're important, and what your role will be.</li> <li>Meet with Anthony to get an overview of the data science and engineering work that has been done to date and where your work will fit in.</li> </ol>"},{"location":"onboarding/new_member_tips/","title":"New Member Tips","text":"<p>We have some recommendations for effective communications on the team. None of these recs are required, nor are they \"one size fits all.\" You should determine which components are relevant for your work (and your team). At minimum, we believe these tips provide examples of good collaborating practices.</p>"},{"location":"onboarding/new_member_tips/#project-check-in-meetings","title":"Project Check-in Meetings","text":"<p>When you're accountable for project updates, here are some tips to keep things moving and build trust.</p> <p>NOTE: This section refers to meeting with supervisors about projects. Separately, we recommend finding time to chat 1:1 about personal growth, interests, and transparency. These 1:1s may benefit from a shared document to track notes and thoughts; however, those discussions probably don't benefit from the (more formal) recommendations below.</p> <p>Regular project meetings are important to provide updates, address blockers, and elicit feedback. This is a lot to cover; typically, there's too much to discuss during the limited meeting time. To help organize project updates, we created a slide template.</p> <p>The slide can be personalized and updated based on your needs. In this process, keep in mind the considerations that the template emphasizes:</p> <ul> <li>Priorities: Which projects and tasks are you responsible or accountable for?<ul> <li>Communicated on the slide by laying out a main project list (rows in table).</li> </ul> </li> <li>Synthesized call-outs: In a time-limited meeting, what are the \u201cbig\u201d ideas or need-to-knows?<ul> <li>This is the \"call-outs\" column in the slide.</li> </ul> </li> <li>Time commitments: How much time do the projects and tasks currently demand?<ul> <li>The \u201cstatus\u201d column contains an opportunity to call out % time.</li> </ul> </li> <li>Status: How are things progressing?<ul> <li>Set a status to good (green), uncertain or behind schedule (yellow), or blocked (red). If you're blocked, bring some ideas about what you need to get back on track.</li> </ul> </li> <li>Feedback: Be prepared to track action items or recommendations during the meeting.<ul> <li>Use the \"meeting comments\" column in the slide to track notes in real time.</li> </ul> </li> </ul>"},{"location":"onboarding/new_member_tips/#after-the-check-in","title":"After the Check-in","text":"<p>Shortly after a project check-in, it's useful to send participants a message which lays out the concrete next steps and prioritization that came out of the meeting. This will help us track decisions, confirm understanding of the next steps and priorities, and will allow us to clarify anything we missed during the meeting. </p>"},{"location":"onboarding/new_member_tips/#schedule-meetings","title":"Schedule Meetings","text":"<p>To save time finding time.</p> <ul> <li>Prompting a meeting: If you've not discussed a new meeting, I'd typically recommend sending a brief note proposing a meeting and its context (see the \"Email Tips\" section below). In your message, you can (a) indicate that they should expect an invite from you shortly, or (b) provide times based on your availability. More info on these options are below.</li> <li>Proposing times: If you\u2019re meeting with people from Yale, use \"Scheduling Assistant\" in Outlook to find available times. (1) Create a new meeting in your calendar, (2) add invitees, (3) click on \"Scheduling Assistant\" in the top bar, and (4) find a time that\u2019s available for everyone.</li> <li>Offering times: If the invitees are external and don\u2019t have assistants to help schedule time, consider using Microsoft Bookings to share your availability. This tool is available through Yale's Microsoft Office Suite, and it stays up-to-date with your Outlook calendar.<ul> <li>Preferably, create a \"personal booking page\" with the length of the desired meeting. You can also set preferences for days or times (beyond your calendar availability). Then copy the link to the booking page and share with your partners.</li> <li>Not only does Bookings allow you to share your availability, it also lets the other party schedule immediately based on their availability.</li> </ul> </li> <li>Agenda: Whenever scheduling a meeting, it is highly recommended to include an agenda. This can be helpful for invitees ... but it's even more important for you as the scheduler. This requires you to reflect on the goals for the meeting: What needs to be covered and what are you trying to accomplish by the end of the meeting?<ul> <li>Several times, I've been close to scheduling the meeting ... but when writing the agenda realized that an email would suffice.</li> </ul> </li> <li>Including a Zoom link: If some or all participants are joining virtually, include a Zoom link in your email. Follow these steps to install the \"Zoom for Outlook\" add-in.</li> </ul>"},{"location":"onboarding/new_member_tips/#example-1-proposing-times","title":"Example 1: Proposing times","text":"<p>Hi team,</p> <p>Based on everyone's availability, it looks like [this time] might work well on Thursday. Let me know if you'd prefer a different time.</p> <p>Please find the agenda and Zoom link below.</p> <p>Thanks!</p> <p>[Your name]</p> <p>Agenda</p> <ul> <li>Review proposed edits</li> <li>Create timeline to finalize draft</li> <li>Delegate remaining tasks</li> </ul>"},{"location":"onboarding/new_member_tips/#example-2-offering-times","title":"Example 2: Offering times","text":"<p>Hi team,</p> <p>We discussed needing additional time on Thursday or Friday to chat before the big deadline. I'm available [your availability]. Just let me know which times work for you all, then I can send a Zoom link.</p> <p>Thanks!</p> <p>[Your name]</p>"},{"location":"onboarding/new_member_tips/#email-tips","title":"Email Tips","text":"<p>This checklist aims to improve team communication \u2026 and help you get responses quicker!</p> <ul> <li>Context: Briefly (1-2 sentences) reintroduce the project or task if recipients have not recently discussed the matter</li> <li>Intent: Make explicit the goal of the message \u2014 request, question, or FYI<ul> <li>For requests, if there are multiple email recipients involved in different aspects of the request, specify what is requested from each individual (e.g., tag \"@\" each requestee)</li> </ul> </li> <li>Timeline: If needed, state whether a response is urgent or not and propose a timeline to resolve requests or questions (while being courteous of others' priorities and availability!)</li> <li>Proposals: Whenever raising complexities or barriers, present potential solutions that the team might consider. These likely belong in the postscript section (see \"details\"). The proposal doesn't need to be perfect; for example, if you don\u2019t have enough context to provide a resolution, you might propose meeting with someone to gather more information as a next step.</li> <li>Attachments: Specify any attachments (sometimes by name if there are multiple files).<ul> <li>If possible (e.g., internal emails with non-sensitive documents), opt to link to a centralized document in Google Drive or Dropbox rather than sending flat files.</li> </ul> </li> <li>Details: For any additional info beyond the core components above, create a separate postscript section after your signature. This helps recipients (a) first understand what\u2019s needed, then (b) refer back to the postscript when setting aside time to address the need.</li> </ul>"},{"location":"onboarding/new_member_tips/#example-1-original-email","title":"Example 1: Original email","text":"<p>Hi team,</p> <p>I\u2019ve run into data issues in the past few days. Does anyone know why I receive the error \"ORA-10023 Index out of Bounds\" when using the MCO data? There\u2019s also the issue of the data formatting in the extracts we received. It\u2019s not allowing me to identify paid dates, which are important for looking at trends like the team requested.</p> <p>Here's a screenshot of the error: (Screenshot.png)</p> <p>Here's what the data looks like in its raw form. (Notice that the paid dates are encoded weirdly.) (Table)</p> <p>I still tried to run some early analyses (see attached Excel), but I'm skeptical of the results because of these issues.</p> <p>Any thoughts on where to go from here?</p> <p>Thank you in advance!</p> <p>[Your name]</p>"},{"location":"onboarding/new_member_tips/#example-2-revised-email","title":"Example 2: Revised email","text":"<p>Hi team,</p> <p>For the MCO project, we had discussed analyzing trends in medical spend. I started pulling data, but encountered some barriers recently (details below). I'm hoping to get some support this week to address the issues.</p> <p>@Colleague_1 with your experience with the data and domain, can you do a 10-15min sanity check of the preliminary findings linked here? Hoping to get feedback by end-of-week if possible, but let me know if that wouldn't work.</p> <p>@Colleague_2 can we set up a peer coding session to identify any programming errors? If you\u2019re open to it, I can send an invite based on your calendar availability over the next couple days.</p> <p>Thank you in advance!</p> <p>[Your name]</p> <p>+++</p> <p>Problem details</p> <ul> <li>Issue #1: The data processing pipeline is failing when I try to merge MCO extracts. Specifically, it\u2019s this line (Github link) with an error: \"...\"</li> <li>Issue #2: Formatting in the raw MCO data extract (\"file_path.csv\") has paid dates with an undefined encoding, which keeps me from running a trend analysis. See a data example below. (Table)</li> </ul>"},{"location":"onboarding/new_member_tips/#seeking-additional-advice","title":"Seeking Additional Advice","text":"<p>Current lab members are here to help. If you have further questions about how the lab operates, don't hesitate to ask.</p>"},{"location":"state_data/server_connect/","title":"Connecting to the Server","text":"<p>You will first need to connect to the VPN (Cisco AnyConnect) via access.yale.edu using your netID. Then you can use Microsoft Remote Desktop to log into the server.</p>"},{"location":"state_data/server_connect/#remote-desktop-configuration","title":"Remote Desktop Configuration","text":"<p>To configure Microsoft Remote Desktop:</p> <ul> <li>Click \"add a PC\"</li> <li>PC Name: <code>ysph-hpm.its.yale.internal</code></li> <li>User account: yale\\your_netID</li> <li>Check \"Reconnect if the connection is dropped\"</li> </ul> <p></p>"},{"location":"state_data/server_connect/#navigating-to-the-project-folder","title":"Navigating to the Project Folder","text":"<p>Due to permissions, you will need to navigate directly to <code>D:\\Groups\\YSPH-HPM-Ndumele\\Networks\\</code>. You can do this by typing that address directly into the file explorer. After navigating there, you can pin this to Quick Access (right click Quick Access and select \"Pin current folder to Quick Access\") for ease in the future (see image below).</p> <p></p>"},{"location":"state_data/state_medicaid_data/","title":"State Medicaid Data","text":""},{"location":"state_data/state_medicaid_data/#overview","title":"Overview","text":"<p>We have Medicaid data extracts from the following states:</p> State Years SNAP Tennessee 2010-2017 N Lousiana 2010-2016 N Connecticut 2015-2021 Y Michigan 2010-2020 N Kansas 2013-2018 N"},{"location":"state_data/state_medicaid_data/#obtaining-access-to-state-data","title":"Obtaining Access to State Data","text":"<p>Once you have obtained access to <code>YSPH_HPM-CC0940-MEDSPH2</code> you will need to log into the server and do the following:</p> <ol> <li>Open File Explorer (the one with the yellow folder icon)</li> <li>Click on This PC on the left side</li> <li>At the top toolbar click on Computer \u2013 Map network drive \u2013 Map Network Drive</li> <li>Keep the default drive letter <code>Y:</code></li> <li>In folder use: <code>\\\\storage.yale.edu\\home\\YSPH_HPM-CC0940-MEDSPH2</code></li> <li>Make sure the checkbox \"Reconnect at sign-in\" is checked</li> <li>Click on finish</li> </ol>"},{"location":"state_data/state_medicaid_data/#data-organization","title":"Data Organization","text":"<p>Data for each state are stored in <code>YSPH_HPM-CC0940-MEDSPH2\\Private Data Pipeline\\Healthcare_Data\\</code>.</p> <p>For all states (except CT) the data are organized as follows:</p> <pre><code>State_Abbreviation/\n\u251c\u2500\u2500 Docs/\n\u251c\u2500\u2500 Raw/\n    \u251c\u2500\u2500 Original/\n        \u251c\u2500\u2500 compressed/\n\u251c\u2500\u2500 Standard/\n\u251c\u2500\u2500 Intermediate/\n\u251c\u2500\u2500 Gold/\n\u251c\u2500\u2500 Analytic_Tables/\n\u251c\u2500\u2500 Logs/\n\u251c\u2500\u2500 Junk/\n</code></pre> <ul> <li><code>Docs/</code>: Contains any data dictionaries or relevant state-specific information.</li> <li><code>Raw/</code>: Contains minimally processed files obtained directly from the state. The only processing is generally to rename files and ensure files are saved in an easy to read format (e.g., .pkl or .parquet).</li> <li><code>Standard/</code>: Contains files that have been initially processed. Typically, data have been typecast and variabled names are mapped to match the standard schema.</li> <li><code>Intermediate/</code>: Contains intermediate files that haven't been fully processed</li> <li><code>Gold/</code>: Contains final files that have been fully processed and match our desired schema. These are generally the starting files used in any research project.</li> <li><code>Analytic_Tables/</code>: Contains any analytic extracts which are derived from the <code>Gold/</code> files. For instance, this folder could contain member-level risk adjustors for each calendar year, annual measures of high- and low-value care, and a basic member-month analytic skeleton.</li> <li><code>Logs/</code>: Contains any log files produced during the processing.</li> <li><code>Junk/</code>: Scrap folder containing any temporary outputs during development.</li> </ul>"},{"location":"tmsis/tmsis_code_collab/","title":"Coding Collaboration","text":"<p>We will use <code>git</code> to collaboratively develop analyses. This vignette will use T-MSIS projects as a working example, but the practices apply elsewhere.</p> <p>On Milgram, the most up to date, stable, version of the libraries will be found in <code>/gpfs/milgram/pi/medicaid_lab/code/</code> on the main branch, while researcher specific development branches can be stored in your own project directory.</p> <p>Always include a .gitignore</p> <p>Code will be pushed remotely to the Yale-Medicaid organization on GitHub. Repositories must be initialized with .gitignore files which ignores:</p> <ul> <li>all common data file extensions (.parquet, .xlsx, .dta, .sas7bdat)</li> <li>files which display data for exploratory analyses (.ipynb)</li> <li>objects that can be used to recreate raw data (.Rdata, targets files)</li> </ul> <p>You can find a template .gitignore here. This file tells git to track the structure of the repository without tracking any individual files in the trunk/ folder.</p>"},{"location":"tmsis/tmsis_code_collab/#set-up","title":"Set up","text":""},{"location":"tmsis/tmsis_code_collab/#quickstart-for-git-on-milgram","title":"Quickstart for Git on Milgram","text":""},{"location":"tmsis/tmsis_code_collab/#generating-ssh-key","title":"Generating SSH Key","text":"<p>This saves you from having to type your password or provide a token every time. YCRC provides a thorough guide on using and generating SSH keys, the main steps of which we outline here. If you've already generated SSH keys (say, to SSH into Milgram through the terminal), you can skip the generation steps. </p> <p>Within a terminal in Milgram type <code>ssh-keygen</code>. Press enter to save the keys in the default location <code>/home/yourusername/.ssh/id_rsa</code>, and create some passphrase as an extra layer of protection. The public key, saved in <code>~/.ssh/id_rsa.pub</code> is what we share. NEVER SHARE THE PRIVATE KEY, which is stored in the similarly named file without the <code>.pub</code> suffix. </p> <p>You can display the contents of the public key with <code>cat ~/.ssh/id_rsa.pub</code>, which will show a very long string beginning with something like <code>ssh-rsa</code>. Copy the entire outut (including the ssh-ra part). </p>"},{"location":"tmsis/tmsis_code_collab/#add-ssh-key-to-github","title":"Add SSH Key to GitHub","text":"<p>Now you need to add that public key to your GitHub Account. Instructions are provided by GitHub.</p> <ol> <li>Click on your account icon</li> <li>Click on Settings</li> <li>Click \"SSH and GPG keys\" under \"Access\" on left hand menu</li> <li>Click \"New SSH Key\"</li> <li>Give it some descriptive name, paste the output from <code>cat ~/.ssh/id_rsa.pub</code> into the Key box</li> <li>Click \"Add SSH Key\"</li> </ol> <p>Now when you clone a repository, you can use the SSH remote URL.</p>"},{"location":"tmsis/tmsis_code_collab/#existing-projects","title":"Existing Projects","text":""},{"location":"tmsis/tmsis_code_collab/#cloning-a-repository","title":"Cloning a Repository","text":"<p>Navigate to the desired repository within Yale-Medicaid on Github. Click on the green <code>&lt;&gt; Code</code> button in the to right and copy the SSH url to your clipboard. </p> <p></p> <p>Within Milgram, use the terminal to navigate to the directory where you'd like to clone this repository. This is very likely <code>/home/NETID/project/</code>. From there type <code>git clone URL_YOU_COPIED_ABOVE</code>, which should begin cloning the directory with messages like:</p> <pre><code>Cloning into 'QualityMetrics'...\nremote: Enumerating objects: 14, done.\nremote: Counting objects: 100% (14/14), done.\nremote: Compressing objects: 100% (11/11), done.\nremote: Total 14 (delta 3), reused 13 (delta 2), pack-reused 0 (from 0)\nReceiving objects: 100% (14/14), done.\nResolving deltas: 100% (3/3), done.\n</code></pre> <p>Navigate into that directory in the terminal. You're likely on the main branch (since you cloned from there), which you can confirm with <code>git branch</code>.</p> <p>Since you don't want to develop on the main branch create a new branch for development and switch to it: <code>git checkout -b NEW_BRANCH_NAME</code>. You should get a message <code>Switched to a new branch 'NEW_BRANCH_NAME'</code>. Try to make the name descriptive if the branch is for a specific task.</p>"},{"location":"tmsis/tmsis_code_collab/#replicating-the-environment","title":"Replicating the Environment","text":""},{"location":"tmsis/tmsis_code_collab/#python","title":"Python","text":"<p>The repository should have an <code>environment.yml</code> file which indicates the packages and exact versions required by the project. We need to ensure everyone is working with the same versions to avoid issues with depricated methods or changes between versions.</p> <p>You can replicate this environment using <code>miniconda</code>. Within Milgram navigate to the project directory, and within a terminal type:</p> <pre><code>module load miniconda\nconda env create --file environment.yml\n</code></pre> <p>This will create an environment with a name based on the <code>name:</code> attribute in the <code>environment.yml</code> file. You need to use this environment when you are testing changes.</p>"},{"location":"tmsis/tmsis_code_collab/#r","title":"R","text":"<p>If you're using <code>renv</code>, open the project in an RStudio Server session and type <code>renv::restore()</code> in the console.</p>"},{"location":"tmsis/tmsis_code_collab/#new-projects","title":"New Projects","text":""},{"location":"tmsis/tmsis_code_collab/#python_1","title":"Python","text":"<p>TODO</p>"},{"location":"tmsis/tmsis_code_collab/#r_1","title":"R","text":"<p>We highly recommend starting your R projects with the template repository. Do this on GitHub, then clone the repository onto Milgram.</p>"},{"location":"tmsis/tmsis_code_collab/#contributing","title":"Contributing","text":""},{"location":"tmsis/tmsis_code_collab/#git-101","title":"Git 101","text":"<p>If you've never used Git, work through GitHub's introduction. When collaborating on code, it's important to stay organized!</p> <p>You can check the status of your current branch with <code>git status</code>. If there are no modified files and everything is up to date you will see:</p> <pre><code>On branch anthony-dev\nYour branch is up to date with 'origin/anthony-dev'.\n\nnothing to commit, working tree clean\n</code></pre> <p>Otherwise, if you've been making changes, in this example creating a new file called <code>test_add.py</code>, <code>git status</code> will indicate the files which have changed or added:</p> <pre><code>Your branch is up to date with 'origin/anthony-dev'.\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n    test_add.py\n\nnothing added to commit but untracked files present (use \"git add\" to track)\n</code></pre> <p>Because this is a new file, we will need to add it to tracking:</p> <pre><code>git add test_add.py\n</code></pre> <p>And when you are ready to commit several changes you would </p> <pre><code>git commit -m \"some commit message\"\n</code></pre> <p>Alternatively, all changes to already tracked files can be commited with </p> <pre><code>git commit -am \"some commit message\"\n</code></pre> <p>Check before you commit</p> <p>Remember to check the files being added and information being committed to ensure no sensitive information is pushed to the remote. </p> <ul> <li>If using <code>-a</code> when committing, take the time to ensure you know exactly which files have changed as this commits <code>all</code> modifications. </li> <li>I recommend against using a command like <code>git add -A</code> to add new files to tracking. Better to explicitly <code>git add file_name</code> as an added layer of security. (The .gitignore is a backstop...)</li> </ul> <p>Finally, committed changes can be pushed to the remote with <code>git push</code>.</p>"},{"location":"tmsis/tmsis_code_collab/#updating-environmentyml","title":"Updating environment.yml","text":"<p>Commit updates to environment.yml alongside code</p> <p>If you install or update a package, environment.yml must be updated in the same commit alongside the code which required the change. This ensures that the version history always has the correct environent capable of running the project without errors. </p> <p>Occassionally you will need to add additional package dependencies or you will want to update packages to more recent versions. When this happens, you need to update <code>environment.yml</code> so others can maintain consistency. To do this, navigate to the project where <code>environment.yml</code> exists and within a terminal:</p> <pre><code>module load miniconda\nconda env export --no-builds --name ENV_NAME | grep -v prefix &gt; environment.yml\n</code></pre> <p>where you replace <code>ENV_NAME</code> with the name of the environment you are looking to export. This will overwrite the <code>environment.yml</code> file, so you can commit any changes alongside the code and push to git. When others pull these changes, they can replicate this environment from the updated file. </p> <pre><code>module load miniconda\nconda env create --file environment.yml\n</code></pre> <p>This will fail if the environment already exists locally, so instead update </p> <pre><code>module load miniconda\nconda env update --file environment.yml --prune\n</code></pre>"},{"location":"tmsis/tmsis_environments/","title":"Environments","text":"<p>Using environments ensures projects remain reproducible in the future. They can also solve complicated issues caused by version incompatability within a project. </p> <p>Environments are nothing more than a list of specific packages used to perform some analysis. While documenting this may seem overkill for a single project, as you work on multiple projects and projects begin to span multiple years it becomes increasingly likely that the packages you use for one project are incompatible with newer versions you used in another. If functions become depracted old code may fail to run with newer versions and often these deprecations are decisions based on stylistic changes, not indications that anything was wrong with those functions. It's often unecessary to retrofit old code because in these cases it's still correct. </p> <p>R and Python can handle environments differently. For python the easist way to manage an environment is through the <code>miniconda</code> module. </p>"},{"location":"tmsis/tmsis_environments/#creating-environments-in-r","title":"Creating Environments in R","text":"<p>This should be pretty straightforward. You should request an RStudio Server Session via the OnDemand portal. More information is available on the YCRC website.</p>"},{"location":"tmsis/tmsis_environments/#creating-environments-in-python","title":"Creating Environments in Python","text":""},{"location":"tmsis/tmsis_environments/#environment-setup","title":"Environment Setup","text":"<p>I recommend setting up environments using the terminal in the remote desktop interactive app. While this can be done via SSH, <code>miniconda</code> sometimes hangs when solving the environment via ssh. </p> <p>Within a terminal on Milgram type: <code>module load miniconda</code>. </p> <p>Next we will create an blank environment initialized with some name. In this case I'm going to call the environment <code>test_env</code>, but you can name it after your specific project.</p> <p><code>conda create -n test_env</code> </p> <p>We could have also added a list of packages to be installed when we created that environment by adding them after the name. For example the following would create an environment named <code>test_env</code> with python3 installed. </p> <p><code>conda create -n test_env python</code></p> <p>After the environment is created you can install additional packages by activating it and using the install command:</p> <p><code>conda activate test_env</code></p> <p>After this step you should notice the terminal prompt changes and now indicates your active environent in parentheses before the path:</p> <p><code>(test_env)[al686@login1.milgram ~]</code></p> <p>Now we can install pacakges:</p> <p><code>conda install numpy matplotlib scipy jupyter pandas pyarrow</code></p> <p>Once you no longer need to use that environment you can leave it with <code>conda deactivate</code>.</p>"},{"location":"tmsis/tmsis_environments/#adding-environment-to-interactive-jupyter-app","title":"Adding Environment to Interactive Jupyter App","text":"<p>When developing code it can be useful to work in an interactive session, but it's important that the environment used in the interactive session is the same one used in your analysis. </p> <p>In a terminal (where you have no yet loaded miniconda) type <code>ycrc_conda_env.sh update</code>.</p> <p>Once this is done, you should be able to select the environment in the dropdown setup when starting an interactive jupyter session.</p> <p></p>"},{"location":"tmsis/tmsis_getting_started/","title":"Getting Started with T-MSIS","text":""},{"location":"tmsis/tmsis_getting_started/#obtaining-access-to-t-msis-data","title":"Obtaining Access to T-MSIS Data","text":"<p>Due to the size and nature of the national data set, T-MSIS data are housed in the HIPAA-aligned Milgram cluster at Yale Center for Research Computing. The first step is to request an account.</p> <ol> <li>Make sure Anthony is aware you are requesting an account as he will need to approve the request.</li> <li>Fill out the form<ul> <li>Department or School: Yale School of Public Health</li> <li>Are you a Principal Investigator: No</li> <li>Principal Investigator: Chima Ndumele</li> <li>Clusters: Milgram</li> <li>Special Requests: request access to the <code>medicaid</code> group.</li> </ul> </li> </ol>"},{"location":"tmsis/tmsis_getting_started/#using-milgram","title":"Using Milgram","text":"<p>Once you've obtained access, sign into Milgram OnDemand. You may need to be connected to the Yale VPN even if you're on the Yale Secure Wi-Fi.</p> <p>There are three ways you'll regularly interact with Milgram.</p>"},{"location":"tmsis/tmsis_getting_started/#terminal","title":"Terminal","text":"<p>You'll use the terminal to run larger jobs and perhaps to explore files. You can open the terminal by clicking \"clusters\" and \"Milgram shell access\" in the OnDemand browser, by opening a Remote Desktop on Milgram OnDemand, or by setting up an SSH connection from your machine.</p> <p>Open the terminal (however you choose) and type <code>groups</code>. You should see <code>medicaid</code>; if you don't try typing <code>groups your_net_id</code>. If you still don't see <code>medicaid</code>, your request for access was not honored and you should contact Anthony. You won't have access to the Medicaid data until your permissions are updated.</p> <p>To make your life easier, you may want to add a symlink at your home directory to the <code>medicaid_lab</code> folder. You can do this by typing <code>ln -s /gpfs/milgram/pi/medicaid_lab /home/your_net_id</code> (replace <code>your_net_id</code>!). This isn't necessary but may be helpful.</p>"},{"location":"tmsis/tmsis_getting_started/#file-explorer","title":"File Explorer","text":"<p>You'll probably want to peruse the Medicaid lab files. Open OnDemand, then click on Files -&gt; Home Directory in the navbar. That should take you to <code>/home/your_net_id/</code>. If you set up the symlink correctly, you should be able to click on <code>medicaid_lab</code>. If not, click \"change directory\" and enter <code>/gpfs/milgram/pi/medicaid_lab/</code>.</p> <p>Don't download files</p> <p>It's easy\u2014too easy\u2014to accidentally begin downloading a file in the explorer. T-MSIS data are only allowed to live on Milgram, so do not download them. Furthermore, don't download any files with information that could possibly recreate, or expose, individual-level claims data. </p> <p>ALWAYS double check jupyter notebooks, log files, error messages, or other exploratory output to ensure there is no individual data being displayed before sharing beyond Milgram.</p> <p>Only connect to Milgram on your official Yale Laptop. These are HIPAA compliant and encrypted as an additional layer of protection if a file download is ever initiated.</p> <p>You can also use the Remote Desktop app to explore files. The interface isn't as nice, but it's good to be careful.</p>"},{"location":"tmsis/tmsis_getting_started/#coding","title":"Coding","text":"<p>Start your first project as a subfolder in <code>/home/your_net_id/project/</code>. Keep in mind that this is actually a symlink to <code>/gpfs/milgram/project/ndumele/your_net_id/</code>.</p> <p>Then, choose your preferred OnDemand app and start coding! Most work at the lab is conducted in R (use RStudio Server) or Python (use Jupyter).</p> <p>For most T-MSIS tasks, the interactive session has enough computing power to work on one year of data in a smaller state, e.g., South Dakota in 2018. When starting your session, you should be able to request six hours of one CPU core with 30 GB of RAM. You can request OnDemand sessions with more cores and/or memory if you use the <code>day</code> or <code>week</code> partition. When you want to run your production analyses, you should submit a job with SLURM.</p>"},{"location":"tmsis/tmsis_getting_started/#accessing-storageyale-on-milgram","title":"Accessing storage@yale on Milgram","text":"<p>Certain drives on storage@yale are also suitable for hisk-risk data, and in rare cases we may need to transfer data between these. To connect to storage@yale on Milgram:</p> <ul> <li>Begin a remote desktop session (Note: the file transfer will need to complete within the allotted time, you can reach out to YCRC to request a long session)</li> <li>Open a file browser, click the pen and pencil icon next to the \"Location\" navigation, type <code>smb://storage.yale.edu/home/</code> and press enter</li> <li>This should open a dialogue box:<ul> <li>Select: registered user   </li> <li>ID: <code>your_NET_ID</code></li> <li>Change SAMBA to YALE</li> <li>password is your normal netID password</li> </ul> </li> <li>Once you log in you should see the shares associated with your account </li> <li>You can open another file browser and \"drag and drop\" files between the two drives</li> </ul>"},{"location":"tmsis/tmsis_getting_started/#t-msis-tech-stack","title":"T-MSIS Tech Stack","text":"<p>The Documentation section in the T-MSIS Medicaid Data page describes the T-MSIS data. After reading that, you'll want to familiarize yourself with the specific tech stack we use at the Medicaid lab.</p> <p>We store all our data in parquet files. R programmers can use the <code>arrow</code> package to connect to these files, and Python programmers can use <code>pyarrow</code>.</p> <p>If you haven't used Arrow or the parquet format before, first read Hadley Wickham's introduction. The exercises are quite helpful. You may find the following material helpful additional reading:</p> <ul> <li>Function reference</li> <li>A workshop notebook</li> <li>An online book by Apache</li> <li>The R <code>arrow</code> package page</li> </ul>"},{"location":"tmsis/tmsis_getting_started/#tips-and-tricks","title":"Tips and Tricks","text":"<ul> <li> <p>When working in a Milgram OnDemand session, you will occasionally need to reauthenticate. This can happen every hour or two, even if you're working the entire time.</p> <p></p> <p>An error saving a file is a big hint that you need to reauthenticate. Click \"Ok,\" then navigate to the Milgram OnDemand \"My Interactive Sessions\" page in another tab. Refresh that page and reauthenticate. You can then re-join your RStudio Server session and get back to work.</p> </li> <li> <p>If you're working on a large task in an RStudio Server session, you may see this error message:</p> <pre><code>Error in `get_result(output = out, options)`: ! callr subprocess failed: could not start R, exited with non-zero status, has crashed or was killed \u2139 See `$stderr` for standard error.\n</code></pre> <p>This often implies that you have not allocated enough memory to the task. You could start a new interactive session with more memory or you can submit a job with SLURM.</p> </li> <li> <p>You may have trouble installing R packages. You can try installing an older version from the CRAN archive, or you can try installing from R Universe.</p> </li> </ul>"},{"location":"tmsis/tmsis_getting_started/#useful-slurm-commands","title":"Useful SLURM Commands","text":"<p>To submit a job, you'll need a shell script that runs your code.</p> <p>You'll generally submit batch jobs using the terminal. <code>cd</code> to the folder with the shell script, then type <code>sbatch submit.sh</code> (assuming your shell script is called <code>submit.sh</code>). You should receive an email from no-reply@milgram.ycrc.yale.edu when the job starts and when it finishes or fails for any reason (error in the code, invalid configuration, ran out of memory, etc.).</p> <p>Most jobs should start immediately. If you still haven't received that \"job started\" email, you're probably in the queue. This is usually a result of you requesting a lot of resources and many other users running their own computationally expensive tasks.</p> <p>First, run <code>squeue --me</code> to make sure your job went through. You should see <code>(Resources)</code> in the right-most column, which indicates that your job is waiting on compute or memory availability.</p> <p>Then, run <code>squeue -p [partition_name]</code> to see who else is using Milgram. (<code>partition_name</code> can be <code>day</code>, <code>week</code>, etc.) Check the Milgram user guide to see the total available compute nodes. If you sum up the existing jobs, you'll probably see that your job would fall over the limits.</p> <p>You can cancel a job if necessary with <code>scancel [job_id]</code>. The cluster will tell you your <code>job_id</code> when you submit it, and you can also find it in your queue. If you lose your <code>job_id</code>, run <code>sacct -X --start now-3hours -o jobid,jobname,start</code>, adjusting <code>3hours</code> to whatever is necessary to capture the start time.</p> <p>To get the stats of a job, run <code>jobstats [job_id]</code>. This is helpful if you want to maximize parallelization with constrained memory.</p>"},{"location":"tmsis/tmsis_getting_started/#finding-available-resources","title":"Finding Available Resources","text":"<p>If your job is stuck in queue, the first thing to consider is if you really need all the resources you requested. If you don't have any parallelization in your code, you probably only need one core. Similarly, if you're not working with large datasets, you shouldn't need 181 GB of memory. Requesting only what you need helps you get your job running sooner and is considerate of others on the cluster.</p> <p>Need what you requested? You might be stuck waiting. If you can reduce your request, continue reading.</p> <p>Some guesswork</p> <p>Some of the information in this subsection may not be entirely accurate. Computing is hard! When in doubt, reach out to the YCRC support desk.</p> <p>When you run <code>squeue -p [partition_name]</code>, look at the values in the <code>NODELIST(REASON)</code> column. Jobs in progress will have a node ID, while queued jobs will have a reason in parentheses, e.g., <code>(Resources)</code>.</p> <p>Say I'm submitting a job requesting one node and all 36 cores, but I'm stuck in the queue. First, I will look at the existing jobs. If all the jobs on one node look like they'll finish soon, perhaps I'll wait in the queue. But if the jobs have another few days, it may be faster to reduce my requested resources.</p> <p>To see how much you need to reduce your resources, run <code>scontrol show node [node_id]</code> with one of the <code>node_id</code> values from <code>squeue -p [partition_name]</code>. The <code>CfgTRES</code> row of the output shows the maximum values, and the <code>AllocTRES</code> shows what's in use. Subtract <code>AllocTRES</code> from <code>CfgTRES</code> to see what's available.</p>"},{"location":"tmsis/tmsis_getting_started/#getting-help","title":"Getting Help","text":"<p>It takes some time to get started, but there are many resources to help. Anthony and other folks at the lab can assist with most questions, which you can ask individually or through the Medicaid lab's T-MSIS Slack channel. Issues with Milgram\u2014including access, permissions, package installation errors, and code execution\u2014can be directed to the YCRC support desk.</p>"},{"location":"tmsis/tmsis_medicaid_data/","title":"T-MSIS TAF RIF","text":"<p>CMS has Transformed Medicaid Statistical Information System (T-MSIS) Analytic Files (TAF) from which they create Reasearch Identifiiable Files (RIF) to meet the needs of researchers. We're just going to coloquially refer to this data as T-MSIS data (though it's technically the TAF RIF). CMS has provided substantial documentation about the files here.</p> <p>After reading this page, check out our getting started guide.</p>"},{"location":"tmsis/tmsis_medicaid_data/#available-data","title":"Available Data","text":"<p>We have Medicaid data for the following time periods:</p> Year Purchased Partitioned Files 2016 Y Y Demo, Out, In, Oth, Rx, LTC, Plan, Prov 2017 Y Y Demo, Out, In, Oth, Rx, LTC, Plan, Prov 2018 Y Y Demo, Out, In, Oth, Rx, LTC, Plan, Prov 2019 Y Y Demo, Out, In, Oth, Rx, LTC, Plan, Prov 2020 Y Y Demo, Out, In, Oth, Rx, LTC, Plan, Prov 2021 Y Y Demo, Out, In, Oth, Rx, LTC, Plan, Prov"},{"location":"tmsis/tmsis_medicaid_data/#data-organization","title":"Data Organization","text":"<p>T-MSIS data initially come as ~1 TB files broken up into several parts. We have partioned the files by year and by state to allow for more manageable chunks of data to be used for initial exploration. The clusters can then scale any analyses to a national level. </p> <p>The starting point for most projects will be the year-by-state partitioned files, located at <code>/gpfs/milgram/pi/medicaid_lab/data/cms/ingested/TMSIS_TAF/</code>. Within this directory there is a folder for each specific file provided by CMS:</p> <pre><code>TMSIS_taf/\n\u251c\u2500\u2500 taf_demog_elig_*/\n\u251c\u2500\u2500 taf_inpatient_header/\n\u251c\u2500\u2500 taf_inpatient_line/\n\u251c\u2500\u2500 taf_inpatient_occurrence/\n\u251c\u2500\u2500 taf_long_term_header/\n\u251c\u2500\u2500 taf_long_term_line/\n\u251c\u2500\u2500 taf_long_term_occurrence/\n\u251c\u2500\u2500 taf_other_services_header/\n\u251c\u2500\u2500 taf_other_services_line/\n\u251c\u2500\u2500 taf_other_services_occurrence/\n\u251c\u2500\u2500 taf_rx_header/\n\u251c\u2500\u2500 taf_rx_line/\n\u251c\u2500\u2500 taf_mngd_care_plan_*/\n\u2514\u2500\u2500 taf_prvdr_*/\n</code></pre> <p>Within each specific file folder the organization is by state, then year:</p> <pre><code>taf_demog_elig_base/\n\u251c\u2500\u2500 year=2016/\n\u2502   \u251c\u2500\u2500 state=AK/\n\u2502   \u2502   \u2514\u2500\u2500 data.parquet\n\u2502   \u251c\u2500\u2500 state=AL/\n\u2502   \u2502   \u2514\u2500\u2500 data.parquet\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 year=2017/\n\u2502   \u251c\u2500\u2500 state=AK/\n\u2502   \u2502   \u2514\u2500\u2500 data.parquet\n\u2502   \u251c\u2500\u2500 state=AL/\n\u2502   \u2502   \u2514\u2500\u2500 data.parquet\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"tmsis/tmsis_medicaid_data/#documentation","title":"Documentation","text":"<p>ResDAC provides extensive documentation for all of the files. We have maintained the original column names and datatypes, so all the documentation presented on this website applies to the standardized files as well as the raw files. The two codebooks offer fantastic data dictionaries.</p> <p>Essential documentation:</p> <ul> <li>User guide (PDF download)</li> <li>Demographic codebook</li> <li>Claims codebook</li> </ul> <p>Additional ResDAC documentation:</p> <ul> <li>Documentation for demographic and eligibility files</li> <li>Documentation for inpatient files</li> <li>Documentation for pharmacy files</li> <li>Documentation for other services files</li> <li>Documentation for long term care files</li> <li>Documentation for annual provider files</li> <li>Documentation for annual plan files</li> <li>Data Quality Atlas</li> </ul>"},{"location":"tmsis/vignettes/enrollment/","title":"Identifying Medicaid &amp; Chip Enrollment","text":"<p>Though the examples below are in Python, similar logic applies when working in R; see the vignette for more information.</p> <p>For nearly every research project, we need to identify which beneficiaries are enrolled in Medicaid and what that coverage entails. Enrollment can be determined solely within the base demographic and eligibility data. While the file contains fields related to the \"days enrolled each month\" we use only the restricted benefits codes to determine eligibility. Taken from DQ Atlas:</p> <p>In many states, once an individual is determined eligible, coverage begins on the first day of the month of the application and generally ends on the last day of the month. Therefore, gaps of less than a month may represent administrative errors or other data quality issues rather than true disenrollment and reenrollment in Medicaid or CHIP in these states.</p> <p>Analyses conducted on the 2016 TAF data found that using the restricted benefits code alone is the most reliable approach for counting the number of Medicaid and CHIP beneficiaries with comprehensive benefits. </p>"},{"location":"tmsis/vignettes/enrollment/#replicating-dq-atlas-monthly-medicaid-chip-enrollment","title":"Replicating DQ Atlas Monthly Medicaid &amp; Chip Enrollment","text":"<p>The demographic and eligibility file have columns for each month (01-12) with suffix <code>_XX</code>. From DQ Atlas, the values for <code>RSTRCTD_BNFTS_CD</code> that determine full Medicaid eligibility are as follows:</p> <p>Indicates the scope of Medicaid or CHIP benefits to which a beneficiary is entitled during the month. Can be used to distinguish between individuals not eligible for Medicaid or CHIP benefits during the month (value of 0); those enrolled with full or comprehensive benefits (values of 1, 4, 5, 7, A, B, or D); a and those enrolled with limited benefits (values of 2, 3, 6, C, E, or F). Beneficiaries with a restricted benefits code of 4 (restricted benefits for pregnancy-related services) have benefits that meet the Minimum Essential Coverage (MEC) requirements in all states except Arkansas, Idaho, and South Dakota. Beneficiaries with a restricted benefits code of 4 in those three states have limited benefits.</p> <p>There is further clarification that code <code>'5'</code> should only be considered in recent years:</p> <p>As of 2020, the restricted benefits code value of 5 (the individual is eligible for Medicaid or Medicaid-Expansion CHIP but, for reasons other than alien, dual-eligibility, or pregnancy-related status, is entitled to restricted benefits only) should be used only if the coverage meets the MEC standard and a new valid value of E should be used if the coverage does not meet the MEC standard. For years prior to 2020, we did not include the code 5 group for any state because it represented a more heterogenous mix of beneficiaries (some of whom had limited benefits in some states).</p> <p>The following code identifies the months of full Medicaid and Chip enrollment and reproduces the \"Avg Mthly Medicaid &amp; CHIP Enrollment in TAF\" column within the Enrollment Benchmarking Table provided by DQ Atlas.</p> <p>On the server, we have created a simple <code>HelperTools</code> directory with some useful variables (like lists of states) so those can be more readily accessed. The example below shows you how to work with that in python.</p> <pre><code>import pandas as pd\nimport numpy as np\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport pyarrow.compute as pc\nimport sys\nsys.path.append('/gpfs/milgram/pi/medicaid_lab/code/HelperTools/')\nfrom DataVariables import StateVars, TMSISPath\n\nfile = 'taf_demog_elig_base'\nyear = 2018\n\n# Generally useful beneficiary columns\nbene_cols = ['BENE_ID', 'STATE_CD', 'AGE', \n             'SEX_CD', 'BIRTH_DT', 'MISG_ELGBLTY_DATA_IND']\n# Columns used to determine eligibility\nelig_cols = [f'RSTRCTD_BNFTS_CD_{str(x).zfill(2)}' for x in range(1,13)]\n\n\n# Loop over all states\nd = {}\nfor state in StateVars().abbrevs:\n    # Read in Data with only required columns\n    table = pq.read_table(TMSISPath().data_path+f'{file}/year={year}/state={state}/data.parquet', \n                          columns=bene_cols+elig_cols)\n\n    # Remove rows for BENE_IDs with claims, but no eligibility info. \n    conds = pc.equal(table['MISG_ELGBLTY_DATA_IND'], 0)\n    table = table.filter(conds)\n\n    # Logic across states and years to define Full Benefit Coverage\n    if state in ['AR', 'ID', 'SD']:\n        codes = ['1', '7', 'A', 'B', 'D']\n    else: \n        codes = ['1', '4', '7', 'A', 'B', 'D']\n    if year &gt;= 2020:\n        codes.append('5')\n\n    # DQ Atlas then calculates the number of cells with a valid code divided by 12\n    # for avg. monthly enrollment. We can do the same with a sum of Booleans:\n    valid_enroll = [pc.is_in(table[col], pa.array(codes)) for col in elig_cols]\n\n    # Save average in the dictionary, round and format numbers.\n    d[StateVars().abbrev_to_name[state]] = f'{np.round(np.sum(valid_enroll)/12, 0):,.0f}'\n\npd.Series(d).sort_index()\n</code></pre> <pre><code>Output:\nAlabama                    947,419\nAlaska                     216,843\nArizona                  1,776,912\nArkansas                   862,029\nCalifornia              12,289,909\n...\nVirginia                 1,059,721\nWashington               1,763,954\nWest Virginia              461,666\nWisconsin                1,043,046\nWyoming                     62,478\ndtype: object\n</code></pre> <p>This takes ~3 minutes to run for a single year and has a minimal memory footprint (&lt;20 GB).</p>"},{"location":"tmsis/vignettes/enrollment/#de-duplicating-multiple-records-for-the-same-beneficiary","title":"De-duplicating Multiple Records for the Same Beneficiary","text":"<p><code>BENE_ID</code> is the unique identifier which tracks an individual across states and across years. However ~3% of records in the base demographic and eligibility file are duplicated with respect to BENE_ID. </p> <p>For demographic information (<code>AGE</code>, <code>BIRTH_DT</code>, <code>SEX_CD</code>) there are very rarely discrepancies across rows for the same <code>BENE_ID</code>. In instances where there is mismatch it largely appears to be typos or small 1-digit transposition issues (e.g., birthdate on one row is '1965-02-18' and '1967-02-18' on another). This is a very small problem (&lt;0.1%) and transposition issues of few days or months are unlikely to meaningfully impact any research. </p> <p>Simple solutions are to groupby the <code>BENE_ID</code> and take the first, last, minimum, maximum, or most commonly reported value. If we want a more nuanced solution for <code>BIRTH_DT</code>, we can exclude only records where the disagreement between the birthdate is larger than some threshold, and then decide which of the similar values to keep.</p> <p>The example below removes any beneficiaries who have a discrepancy in <code>BIRTH_DT</code> of more than 1 year and who have conflicting <code>SEX_CD</code> information. We then determine demographic values as the minimum <code>BIRTH_DT</code> and the singular <code>SEX_CD</code> for that <code>BENE_ID</code>. </p> <pre><code># Example using Pennsylvania 2018\nstate = 'PA'\nyear = 2018\nfile = 'taf_demog_elig_base'\nbene_cols = ['BENE_ID', 'STATE_CD', 'AGE', \n             'SEX_CD', 'BIRTH_DT', 'MISG_ELGBLTY_DATA_IND']\n\n# Read in data and filter to valid rows\ndf = pd.read_parquet(TMSISPath().data_path+f'{file}/year={year}/state={state}/data.parquet', \n                     columns=bene_cols,\n                     filters=[('MISG_ELGBLTY_DATA_IND', '=', 0)])\n\n# Remove rows with missing beneficiary identifier\ndf = df[df['BENE_ID'].notnull()]\n\n# Begin reconciling differences\ndf['BIRTH_DT'] = pd.to_datetime(df['BIRTH_DT'], errors='coerce')\n\n# Calculate difference in days, exclude above threshold\ns = df.groupby('BENE_ID')['BIRTH_DT'].agg(['min', 'max'])\ns = (s['max'] - s['min']).dt.days\nTHRESHOLD = 365\nto_exclude = s[s.ge(THRESHOLD)].index\ndf = df[~df.BENE_ID.isin(to_exclude)]\n\n# Age, remove if there are multiple unique values\ns = df.groupby('BENE_ID')['SEX_CD'].nunique()\nto_exclude = s[s.gt(1)].index\ndf = df[~df.BENE_ID.isin(to_exclude)]\n\n# Obtain resolved beneficiary information\ndf_bene = df.groupby('BENE_ID').agg(BIRTH_DT=('BIRTH_DT', 'min'),\n                                    SEX_CD=('SEX_CD', 'first'))\n</code></pre>"},{"location":"tmsis/vignettes/identifying_ED_visits/","title":"Identifying ED Visits in T-MSIS","text":"<p>Though the examples below are in Python, similar logic applies when working in R; see the vignette for more information.</p> <p>More to come...</p>"},{"location":"tmsis/vignettes/providers/","title":"Identifying Providers and Facilities","text":"<p>Though the examples below are in Python, similar logic applies when working in R; see the vignette for more information.</p> <p>It's not immediately clear how to identify providers and facilities in the T-MSIS data. Though imperfect, we use the servicing and billing provider NPIs in our work.</p> <p>This vignette is inspired by our work on the T-MSIS Other Services file. YMMV on the other T-MSIS datasets.</p>"},{"location":"tmsis/vignettes/providers/#npi-dataset","title":"NPI Dataset","text":"<p>The Centers for Medicare and Medicaid Services publish information about each provider through the National Plan and Provider Enumeration System (NPPES) Downloadable File, which is updated monthly. You can learn more about a single NPI using the CMS NPI registry.</p> <p>A repository in the Yale Medicaid GitHub organization cleans the dataset and converts the output into a parquet file. The most updated cleaned dataset is located at <code>/gpfs/milgram/pi/medicaid_lab/data/public/nppes/ingested/todate</code> on Milgram. The monthly cleaned datasets are located at <code>/gpfs/milgram/pi/medicaid_lab/data/public/nppes/ingested/npi_[YYYYMM]</code>.</p> <p>The cleaned dataset contains the provider NPI, credential, gender, taxonomy information (though we suggest using taxonomy values from NUCC, located at <code>/gpfs/milgram/pi/medicaid_lab/data/public/nucc_taxonomy/ingested/nucc_taxomony/</code> on Milgram), and an \"entity type code\" that distinguishes individuals from organizations. The entity type code can be <code>\"individual\"</code>, <code>\"organization\"</code>, or missing.</p>"},{"location":"tmsis/vignettes/providers/#providers","title":"Providers","text":"<p>The <code>SRVC_PRVDR_NPI</code> column is available in the IP, LT, and OT line files. The servicing provider NPI gives the \"National Provider Identifier (NPI) of the health care professional who delivers or completes a particular medical service or non-surgical procedure,\" according to the documentation.</p> <p>In our research on emergency departments, we join the NPPES NPI dataset into our claims dataset using the <code>SRVC_PRVDR_NPI</code> column. Then, we filter the entity type code to <code>\"individual\"</code>. Unfortunately, less than half of our claims in the T-MSIS Other Services file have an <code>\"individual\"</code> assigned at the servicing provider, so we cannot identify an actual doctor for the remaining majority of the claims. We've noticed that whenever a servicing provider has entity type <code>\"organization\"</code>, the servicing NPI is identical to the billing NPI; that is, the facility overall is the \"health care professional\" who provides service.</p> <p>Though the servicing provider is assigned at each line on a claim, we've found that a single claim often has the same <code>SRVC_PRVDR_NPI</code> value on each line. This isn't universal, though, so you should take care if selecting a \"primary\" <code>SRVC_PRVDR_NPI</code>.</p>"},{"location":"tmsis/vignettes/providers/#facilities","title":"Facilities","text":"<p>It's easier to identify a facility, at least in the Other Services file. The <code>BLG_PRVDR_NPI</code> column is the \"National Provider ID (NPI) of the billing entity responsible for billing a patient for healthcare services. The billing provider can also be servicing, referring, or prescribing provider. Can be admitting provider except for Long Term Care,\" according to the documentation. The <code>BLG_PRVDR_NPI</code> column appears in the IP, LT, OT, and RX header files.</p> <p>We've found the vast majority (98-99%) of <code>BLG_PRVDR_NPI</code> entity type codes are <code>\"organization\"</code>, which is what we want. Again, this may not be true outside the Other Services file.</p> <p>Also keep in mind that some facilities may have multiple NPI values. This can provide greater granularity to identify parts of a hospital, but it may affect any facility-level aggregations.</p>"},{"location":"tmsis/vignettes/working_in_R/","title":"Working in R","text":"<p>The <code>arrow</code> package in R opens a connection to a dataset without reading it into memory. Core functions from <code>dplyr</code> and some other packages are translated into Arrow, then executed in C++ to cut runtime by as much as 100x.</p> <p><code>arrow</code> also works nicely with <code>duckdb</code>, a database language that can work with even larger datasets. I occassionally send data to <code>duckdb</code> (using <code>arrow::to_duckdb()</code>) to run filters on large datasets, then bring it back with <code>arrow::to_arrow()</code> \u2014 no need to actually create a database.</p> <p>In general, you should do as much processing as possible before pulling data into R. Be creative! Some functions you ordinarily use might not be translatable into Arrow \u2014 you might have to re-implement it with simple <code>dplyr</code> commands.</p>"},{"location":"tmsis/vignettes/working_in_R/#opening-datasets-with-arrow","title":"Opening Datasets with Arrow","text":"<p>A \"dataset\" is a collection of parquet files. To open a single-file dataset stored at <code>path_to_data/my_data.parquet</code>, run <code>arrow::open_dataset(\"path_to_data\")</code>.</p> <p>This generalizes to a hive-partitioned dataset, like our T-MSIS TAF data. If you want to read every state's 2016 inpatient header, you can run <code>arrow::open_dataset(\"/gpfs/milgram/pi/medicaid_lab/data/cms/ingested/TMSIS_taf/taf_inpatient_header/year=2016/\")</code>. Arrow will tell you that you opened a dataset with 52 files. Also notice that Arrow creates a column called <code>state</code> to reflect the partitioning. If you read every year and every state (with <code>arrow::open_dataset(\"/gpfs/milgram/pi/medicaid_lab/data/cms/ingested/TMSIS_taf/taf_inpatient_header/\")</code>), Arrow would also create a <code>year</code> column.</p> <p>Sometimes opening a hive-partitioned dataset can cause some headaches. Suppose you have a character-type <code>year</code> column in each dataset already. If you run <code>arrow::open_dataset(\"/gpfs/milgram/pi/medicaid_lab/data/cms/ingested/TMSIS_taf/taf_inpatient_header/\")</code>, Arrow will try to also add an integer <code>year</code> column, and you'll hit an error. If this happens, you can instead pass a vector of parquet files to <code>arrow::open_dataset()</code>:</p> <pre><code>list.files(\"/gpfs/milgram/pi/medicaid_lab/data/cms/ingested/TMSIS_taf/taf_inpatient_header/\",\n           pattern = \"*.parquet\",\n           full.names = TRUE,\n           recursive = TRUE) |&gt;\n  arrow::open_dataset()\n</code></pre>"},{"location":"tmsis/vignettes/working_in_R/#pivoting-in-arrow","title":"'Pivoting' in Arrow","text":"<p><code>tidyr::pivot_longer()</code> is not yet available in Arrow. This can be disruptive, but you should be able to work around it. If your dataset isn't too large, do all the processing you can, pull the data into R, and pivot. However, if your dataset is still huge, you can manually pivot in Arrow.</p> <p>Think about what a pivot does \u2014 it stacks multiple columns into one dataset. Suppose your dataset has columns ID, A, B, and C. If you want to stack columns A, B, and C, you can save three intermediate datasets with columns ID, name, and value, where name is \"A\" and value is the value from column A (and the same for B and C). Then, read those three files into a new dataset.</p>"},{"location":"tmsis/vignettes/working_in_R/#anonymous-functions-in-arrow","title":"Anonymous Functions in Arrow","text":"<p><code>dplyr</code> has a fantastic <code>dplyr::across()</code> function to let you run functions on multiple columns. This is implemented in Arrow, but only for named functions, i.e., you cannot pass an anonymous function. The workaround is easy \u2014 simply name your function!</p> <pre><code># DOESN'T WORK\n\ndf |&gt;\n  dplyr::mutate(dplyr::across(some_columns, \\(x) !is.na(x)))\n\n# WORKS\n\nnot_is.na &lt;- function(x) {\n  !is.na(x)\n}\n\ndf |&gt;\n  dplyr::mutate(dplyr::across(some_columns, not_is.na))\n</code></pre> <p>It seems a bit silly, but that's what works right now.</p>"},{"location":"tmsis/vignettes/working_in_R/#other-tips-and-tricks","title":"Other Tips and Tricks","text":"<ul> <li>Arrow enforces separate data types for <code>int32</code> and <code>int64</code>. If your joins don't work, this could be why!</li> <li>You can join a tibble into an Arrow dataset, but not the other way around.</li> <li>You can't run <code>dplyr::bind_rows()</code> in Arrow. Instead, save each piece as a parquet file and open them together.</li> </ul>"},{"location":"tmsis/vignettes/working_with_file_sizes/","title":"Working with File Sizes","text":"<p>T-MSIS TAF data are large, and at the start of the project it can be overwhelming and technically challenging to manipulate the data. The data are already organized into more manageable year-by-state partitions, but larger states (CA, NY, TX) and larger files (Other Services) still exceed hundreds of GB, if not TB, when read into memory (the <code>.parquet</code> files are highly compressed and can be 1-2 orders of magnitude off of the true in memory footprint).</p> <p>Below we provide some tips and worked examples that will help get a project started before data are subset to a more manageable size.</p> <p>Though the examples below are in Python, similar logic applies when working in R; see the vignette for more information.</p>"},{"location":"tmsis/vignettes/working_with_file_sizes/#reading-data","title":"Reading Data","text":""},{"location":"tmsis/vignettes/working_with_file_sizes/#dataframes","title":"DataFrames","text":"<p>For small states and small files it's reasonable to read the entire dataset into memory. Below we read the entire California Inpatient Header file for 2020 into a pandas DataFrame, which requires ~7 GB of memory.</p> <pre><code>import pandas as pd\n\ndata_p = '/gpfs/milgram/pi/medicaid_lab/data/cms/ingested/TMSIS_taf/'\nfile = 'taf_inpatient_header/year=2020/state=CA/data.parquet'\n\ndf = pd.read_parquet(data_p+file)\n\ndf.info(memory_usage='deep')\n#memory usage: 6.4 GB\n</code></pre> <p>While this can be helpful for initial exploratory analyses, once you have a better sense of the types of analyses and relevant data you should use the <code>columns</code> argument to read in only the relevant data. Because <code>.parquet</code> files are column-organized this can be done efficiently. Below we'll read in the standard important claims information (Date, ID, Member) as well as the diagnosis information (admitting diagnosis + 12 potential diagnosis columns). The result is a much smaller DataFrame that only takes up 1/4 of the previous memory requirement. </p> <pre><code>columns = (['BENE_ID', 'CLM_ID', 'SRVC_END_DT', \n            'ADMTG_DGNS_CD', 'ADMTG_DGNS_VRSN_CD']\n            + [f'DGNS_CD_{x}' for x in range(1,13)]\n            + [f'DGNS_VRSN_CD_{x}' for x in range(1,13)])\n\ndf = pd.read_parquet(data_p+file, columns=columns)\n\ndf.info(memory_usage='deep')\n#memory usage: 1.6 GB\n</code></pre>"},{"location":"tmsis/vignettes/working_with_file_sizes/#arrow-tables","title":"Arrow Tables","text":"<p>Once the files become large enough the memory footprint of DataFrames can become problematic. Arrow tables have a much smaller footprint and can handle basic filtering and merging operations. This allows you to read in a larger amounts of data (compared with DataFrames) and perform merge or filtering operations to a point where it's practical to then use <code>pandas</code> or <code>dyplr</code> for more sophisticated manipulations.</p> <p>While the pandas DataFrame above had a 6.4 GB memory footprint, the comparable arrow table only takes up 1.3 GB.</p> <pre><code>import pyarrow.parquet as pq\n\ndata_p = '/gpfs/milgram/pi/medicaid_lab/data/cms/ingested/TMSIS_taf/'\nfile = 'taf_inpatient_header/year=2020/state=CA/data.parquet'\n\ntbl = pq.read_table(data_p+file)\n\nprint(f'memory usage: {tbl.nbytes/10**9:.1f} GB')\n#memory usage: 1.3 GB\n</code></pre> <p>Arrow also supports a <code>columns</code> argument; now, the table subset to diagnosis information barely takes up 300 MB:</p> <pre><code>tbl = pq.read_table(data_p+file, columns=columns)\n\nprint(f'memory usage: {tbl.nbytes/10**9:.1f} GB')\n#memory usage: 0.3 GB\n</code></pre> <p>At any time a pyarrow table can be converted to a pandas DataFrame using the <code>.to_pandas()</code> method:</p> <pre><code>df = tbl.to_pandas()\n\n# Though now we're back to the same, larger footprint above\ndf.info(memory_usage='deep')\n#memory usage: 1.6 GB\n</code></pre>"},{"location":"tmsis/vignettes/working_with_file_sizes/#filtering-data","title":"Filtering Data","text":""},{"location":"tmsis/vignettes/working_with_file_sizes/#background","title":"Background","text":"<p>Often we only need a subset of the data, which can be easily identified from some field(s) within the data. In these cases <code>filters</code> can be provided to the read operations which can greatly reduce the overall memory footprint. </p> <p>For instance, reading in just 3 fields from the New York Other Services Header file for 2020 runs out of memory (even with 180 GB allocated) for the arrow table! It simply is too large to work with...</p> <pre><code>import pyarrow.parquet as pq\n\ndata_p = '/gpfs/milgram/pi/medicaid_lab/data/cms/ingested/TMSIS_taf/'\nfile = 'taf_other_services_header/year=2020/state=NY/data.parquet'\n\ncolumns = ['BENE_ID', 'CLM_ID', 'POS_CD']\ntbl = pq.read_table(data_p+file)\n#memory usage: &gt; 180 GB (OOM event kills it)\n</code></pre> <p>If we only need a specific subset of data \u2014 e.g., only emergency departments, sometimes defined by <code>POS_CD == '23'</code> \u2014 then we can add this to the read to greatly reduce the data. </p> <pre><code>tbl = pq.read_table(data_p+file,\n                    filters=[('POS_CD', '=', '23')])\n\nprint(f'memory usage: {tbl.nbytes/10**9:.1f} GB')\n#memory usage: 2.6 GB\n</code></pre> <p>Now the data are small enough that we can also bring in more columns and easily manipulate it. </p>"},{"location":"tmsis/vignettes/working_with_file_sizes/#simple-examples","title":"Simple Examples","text":"<p>Both <code>pandas.read_parquet</code> and <code>pyarrow.parquet.read_table</code> support the same filtering. Filtering based on the following comparisons: <code>[==, =, &gt;, &gt;=, &lt;, &lt;=, !=, in, not in]</code> is achieved by providing a list of tuples, which each tuple being <code>(column_name, comparison, value)</code>. </p> <p>Below we filter to inpatient hospital admissions for a delivery (diagnosis code Z3800):</p> <pre><code>import pandas as pd\nimport pyarrow.parquet as pq\n\ndata_p = '/gpfs/milgram/pi/medicaid_lab/data/cms/ingested/TMSIS_taf/'\nfile = 'taf_inpatient_header/year=2020/state=CA/data.parquet'\n\ntbl = pq.read_table(data_p+file,\n                    filters=[('ADMTG_DGNS_CD', '==', 'Z3800')]\n                    )\n</code></pre> <p>Additional conditions can be joined with <code>AND</code> logic by providing additional tuples within this list. The following filters to deliveries that occur within January of 2020.</p> <pre><code>tbl = pq.read_table(data_p+file,\n                    filters=[('ADMTG_DGNS_CD', '==', 'Z3800'),\n                             ('SRVC_END_DT', '&lt;=', pd.to_datetime('2020-01-31')),\n                             ('SRVC_END_DT', '&gt;=', pd.to_datetime('2020-01-01'))]\n                   )\n</code></pre> <p><code>OR</code> logic works by embedding separate lists within the overall <code>filters</code> list. To include either deliveries (Z3800) OR cesarean deliveries (Z3801):</p> <pre><code>tbl = pq.read_table(data_p+file,\n                    filters=[\n                             [('ADMTG_DGNS_CD', '==', 'Z3800')], # Delivery\n                             [('ADMTG_DGNS_CD', '==', 'Z3801')]  # Or C-Section\n                            ])\n\n# Equivalently, you would really use `in` for this.\ntbl = pq.read_table(data_p+file,\n                    filters=[('ADMTG_DGNS_CD', 'in', ['Z3800', 'Z3801'])]\n                    )                                           \n</code></pre> <p>Naturally, these can be combined. If you want logic that identifies (Deliveries AND January) OR (C-sections AND April):</p> <pre><code>tbl = pq.read_table(data_p+file,\n                    filters=[\n                             [('ADMTG_DGNS_CD', '==', 'Z3800'),\n                              ('SRVC_END_DT', '&lt;=', pd.to_datetime('2020-01-31')),\n                              ('SRVC_END_DT', '&gt;=', pd.to_datetime('2020-01-01'))], # Delivery &amp; January\n                                                                                    # OR\n                             [('ADMTG_DGNS_CD', '==', 'Z3801'),\n                              ('SRVC_END_DT', '&lt;=', pd.to_datetime('2020-04-30')),\n                              ('SRVC_END_DT', '&gt;=', pd.to_datetime('2020-04-01'))] # C-Section &amp; April\n                            ])\n\n# Simple verification\ndf = tbl.to_pandas()\nprint(df.groupby('ADMTG_DGNS_CD').SRVC_END_DT.agg(['min', 'max']))                      \n#                      min         max\n#ADMTG_DGNS_CD                        \n#Z3800          2020-01-01  2020-01-31\n#Z3801          2020-04-01  2020-04-30\n</code></pre>"},{"location":"tmsis/vignettes/working_with_file_sizes/#advanced-filtering","title":"Advanced Filtering","text":"<p>TODO</p>"},{"location":"tmsis/vignettes/working_with_file_sizes/#partition-strategy-for-large-files","title":"Partition Strategy for Large Files","text":""},{"location":"tmsis/vignettes/working_with_file_sizes/#background_1","title":"Background","text":"<p>In certain cases none of the above options work. If you need to create medical per-member-per-month spending by category in a state like California you'll need to read in the entire Other Services file and many of individual columns.</p> <p>In these cases we can impose further partitioning based on the last digit(s) of the scrambled <code>BENE_ID</code> column. This is quite a powerful \"trick\" because the following are true:</p> <ul> <li><code>BENE_ID</code> is already present and (almost) entirely non-missing in all files.</li> <li><code>BENE_ID</code> is scrambled so this evenly divides the data. </li> <li>Almost every analysis will link data within a person not across people. </li> </ul> <p>To expand on that last point, a typical scenario is merging line files with their corresponding header file with a join on <code>CLM_ID</code>. If headers and lines are randomly partitioned then we still need to compare each header partition with every line partition since we don't know where the matches will occur. However, because claims are unique to a single person, if we partition the header files (based on the last character of <code>BENE_ID</code>) then we know we only need to compare the header partition that ends with <code>'A'</code> to the line partition that ends with <code>'A'</code> (claims for different people by definition have different <code>CLM_ID</code> values).</p> <p>There are other options</p> <p>In the above example, you might be thinking \"why partition on <code>BENE_ID</code> when you could partition on <code>CLM_ID</code>\", which is the merge key and guarantees the joins happen within partition. That would also be a perfectly fine strategy, though in practice we often perform multiple merges or have more complicated join conditions. If in addition to merging header and line files, you'd also like to bring over demographic and eligibility data, then <code>BENE_ID</code> becomes an obvious choice for a partition which guarantees the within-partition relationship between data. </p> <p>Still, in other cases <code>BENE_ID</code> may not be the right choice. If analyses happen across beneficiaries, say identifying members linked through a case (<code>MSIS_CASE_NUM</code>), then you could partition based on this field. As this field is not readily available in every file you'd need to map <code>BENE_ID</code> in each file to <code>MSIS_CASE_NUM</code> to respect that partitioning strategy.</p> <p>We have pre-partitioned the Other Services file by the last character of the claim ID, which you can find at <code>/gpfs/milgram/pi/medicaid_lab/data/cms/ingested/TMSIS_super_partitions</code>.</p>"},{"location":"tmsis/vignettes/working_with_file_sizes/#code","title":"Code","text":"<p>TODO </p>"}]}